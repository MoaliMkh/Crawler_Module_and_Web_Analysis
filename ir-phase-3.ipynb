{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز سوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    در این فاز از پروژه، تمرکز ما بر \n",
    "    crawling \n",
    "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
    "    web crawling\n",
    "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
    "    <br>\n",
    "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
    "    PageRank\n",
    "    و \n",
    "    HITS\n",
    "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
    "    <br>\n",
    "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
    "    <br>\n",
    "در نهایت، ما یک \n",
    "    task \n",
    "     در مورد \n",
    "    recommendation system \n",
    "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler \n",
    "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
    "    ۱۰\n",
    "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
    "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
    "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
    "</li>\n",
    "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
    "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
    "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "url_frontier_kasaei = []\n",
    "url_frontier_sharifi = []\n",
    "url_frontier_rabiee = []\n",
    "url_frontier_rohban = []\n",
    "url_frontier_soleymani = []\n",
    "\n",
    "\n",
    "\n",
    "with open('Sharifi.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        url_frontier_sharifi.append(line[38:])\n",
    "\n",
    "with open('Kasaei.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        url_frontier_kasaei.append(line[38:])\n",
    "\n",
    "with open('Rabiee.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        url_frontier_rabiee.append(line[38:])\n",
    "\n",
    "with open('Rohban.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        url_frontier_rohban.append(line[38:])\n",
    "\n",
    "with open('Soleymani.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        url_frontier_soleymani.append(line[38:])\n",
    "\n",
    "all_url_frontiers = []\n",
    "all_url_frontiers.append(url_frontier_sharifi)\n",
    "all_url_frontiers.append(url_frontier_soleymani)\n",
    "all_url_frontiers.append(url_frontier_rohban)\n",
    "all_url_frontiers.append(url_frontier_rabiee)\n",
    "all_url_frontiers.append(url_frontier_kasaei)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10_refs_for_each_paper(ref_count, flag, references_flag):\n",
    "    list_of_all_refs_for_paper = []\n",
    "    count = 1\n",
    "    while True:\n",
    "        if count == 11 or count == ref_count:\n",
    "            break\n",
    "        else:\n",
    "            if references_flag:\n",
    "                ith_ref_page = f'//*[@id=\"main-content\"]/div[3]/div/div[2]/div[2]/div/div[1]/div[{count}]'\n",
    "                            \n",
    "                ith_ref = driver.find_element(\"xpath\", ith_ref_page).get_attribute('data-paper-id')\n",
    "                list_of_all_refs_for_paper.append(ith_ref)\n",
    "                count += 1\n",
    "            \n",
    "            else:\n",
    "                if flag:\n",
    "                    ith_ref_page = f'//*[@id=\"main-content\"]/div[3]/div/div[3]/div[2]/div/div[1]/div[{count}]'\n",
    "                    ith_ref = driver.find_element(\"xpath\", ith_ref_page).get_attribute('data-paper-id')\n",
    "                    list_of_all_refs_for_paper.append(ith_ref)\n",
    "                    count += 1\n",
    "                else:\n",
    "                    ith_ref_page = f'//*[@id=\"main-content\"]/div[3]/div/div[2]/div[2]/div/div[1]/div[{count}]'\n",
    "                                \n",
    "                    ith_ref = driver.find_element(\"xpath\", ith_ref_page).get_attribute('data-paper-id')\n",
    "                    list_of_all_refs_for_paper.append(ith_ref)\n",
    "                    count += 1\n",
    "    return list_of_all_refs_for_paper\n",
    "\n",
    "def getText(element):\n",
    "    if element:\n",
    "        return element.text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "crawled_data = []\n",
    "\n",
    "def crawl(list_of_output_address, COUNT=2000):\n",
    "\n",
    "    crawled_id = {}\n",
    "\n",
    "    driver.set_page_load_timeout(10)\n",
    "\n",
    "    title_xpath = '//*[@id=\"main-content\"]/div/div/div/div/div/h1'\n",
    "    abstract_xpath = '//*[@id=\"main-content\"]/div/div/div/div/div/div/div/span'\n",
    "    pub_year_xpath = '//*[@id=\"main-content\"]/div/div/div/div/div/ul[2]/li[2]'\n",
    "    author_xpath = '//*[@id=\"main-content\"]/div/div/div/div/div/ul[2]/li[1]'\n",
    "    related_topics_xpath = '//*[@id=\"main-content\"]/div/div/div/div/div/ul[2]/li[3]'\n",
    "    flag = True\n",
    "    references_flag = True\n",
    "\n",
    "\n",
    "    \n",
    "    while COUNT > 0:\n",
    "        for index, url_frontier in enumerate(all_url_frontiers):\n",
    "            specific_crawl_data = []\n",
    "            temp_count = 400\n",
    "            while temp_count > 0:\n",
    "            \n",
    "                try:\n",
    "                    id = url_frontier[0]\n",
    "                    print(\"Page number with id \" + id + \" is being fetched\")\n",
    "                    try:\n",
    "                        driver.get('https://www.semanticscholar.org/paper/' + id)\n",
    "            \n",
    "                    except TimeoutException:\n",
    "                        driver.execute_script(\"window.stop();\")\n",
    "                    \n",
    "                    # pbar.update(1)\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    if 'Tables' in driver.find_element(\"xpath\", '//*[@id=\"main-content\"]/div[2]/div/div/nav/div/ul/li[1]/a').text or \"Figures\" in driver.find_element(\"xpath\", '//*[@id=\"main-content\"]/div[2]/div/div/nav/div/ul/li[1]/a').text:\n",
    "                        flag = True\n",
    "                        ref_count_xpath = '//*[@id=\"main-content\"]/div[2]/div/div/nav/div/ul/li[3]/a'\n",
    "                        cite_count_xpath = '//*[@id=\"main-content\"]/div[2]/div/div/nav/div/ul/li[2]/a'\n",
    "                    else:\n",
    "                        flag = False\n",
    "                        ref_count_xpath = '//*[@id=\"main-content\"]/div[2]/div/div/nav/div/ul/li[2]/a'\n",
    "                        cite_count_xpath = '//*[@id=\"main-content\"]/div[2]/div/div/nav/div/ul/li[1]/a'\n",
    "\n",
    "                    \n",
    "\n",
    "                    if \"References\" in driver.find_element(\"xpath\", '//*[@id=\"main-content\"]/div[2]/div/div/nav/div/ul/li[2]/a').text:\n",
    "                        references_flag = True\n",
    "                    else:\n",
    "                        references_flag = False\n",
    "                        \n",
    "\n",
    "                    reference_count = driver.find_element(\"xpath\", ref_count_xpath).text.split(' ')[0]\n",
    "                    cite_count = driver.find_element(\"xpath\", cite_count_xpath).text.split(' ')[0]\n",
    "                    try:\n",
    "                        reference_count = int(reference_count)\n",
    "                    except:\n",
    "                        reference_count = 10\n",
    "\n",
    "                    try:\n",
    "                        cite_count = int(cite_count)\n",
    "                    except:\n",
    "                        cite_count = 10\n",
    "\n",
    "\n",
    "                    data = {\n",
    "                        \"Title\": driver.find_element(\"xpath\", title_xpath).text,\n",
    "                        \"ID\": id,\n",
    "                        \"pub_year\" : driver.find_element(\"xpath\", pub_year_xpath).text.split(' ')[-1],\n",
    "                        \"Authors\": driver.find_element(\"xpath\", author_xpath).text,\n",
    "                        \"Related Topics\": driver.find_element(\"xpath\", related_topics_xpath).text,\n",
    "                        \"Citation Count\": cite_count,\n",
    "                        \"Reference Count\": reference_count,\n",
    "                    }\n",
    "\n",
    "                    if len(driver.find_elements(\"xpath\", abstract_xpath)) != 0:\n",
    "                        data['Abstract'] = driver.find_element(\"xpath\", abstract_xpath).text\n",
    "                    else:\n",
    "                        data['Abstract'] = \"\"\n",
    "\n",
    "                    if isinstance(reference_count, int):\n",
    "                        data['References'] = get_10_refs_for_each_paper(reference_count, flag, references_flag)\n",
    "                    else:\n",
    "                        try:\n",
    "                            data['References'] = get_10_refs_for_each_paper(10, flag, references_flag)\n",
    "                        except:\n",
    "                            data['References'] = \"No References\"\n",
    "\n",
    "\n",
    "\n",
    "                    COUNT -= 1\n",
    "                    temp_count -= 1\n",
    "                    print(\"Page number with id \" + id + \" is ready\")\n",
    "                    # print(str(COUNT) + ' page remain')\n",
    "                    url_frontier.pop(0)\n",
    "                    crawled_data.append(data)\n",
    "                    specific_crawl_data.append(data)\n",
    "                    crawled_id[id] = True\n",
    "                    for id in data['References']:\n",
    "                        if not crawled_id.__contains__(id):\n",
    "                            crawled_id[id] = True\n",
    "                            url_frontier.append(id)\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print('unknown Exception occurred')\n",
    "                    url_frontier.pop(0)\n",
    "                    continue\n",
    "\n",
    "            print('crawling done for this professor, writing info file')\n",
    "            \n",
    "            with open(list_of_output_address[index], 'w') as f:\n",
    "                json.dump(specific_crawl_data, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    print('crawling done writing info file')\n",
    "    with open('all_crawled_data.json', 'w') as f:\n",
    "        json.dump(crawled_data, f, indent=4)\n",
    "    return crawled_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page number with id Inhibition-of-TGFβ-Signaling-Promotes-Ground-State-Hassani-Totonchi/8d6835be604459b317fd2f26f9db4a6118586740\n",
      " is being fetched\n",
      "Page number with id Inhibition-of-TGFβ-Signaling-Promotes-Ground-State-Hassani-Totonchi/8d6835be604459b317fd2f26f9db4a6118586740\n",
      " is ready\n",
      "Page number with id Treatment-of-human-embryonic-stem-cells-with-of-and-Tahamtani-Azarnia/788db0c4d5b98e835027cc2331419bf6b26f1c77\n",
      " is being fetched\n",
      "Page number with id Treatment-of-human-embryonic-stem-cells-with-of-and-Tahamtani-Azarnia/788db0c4d5b98e835027cc2331419bf6b26f1c77\n",
      " is ready\n",
      "Page number with id Graph-Traversal-Edit-Distance-and-Extensions-Boroojeny-Shrestha/0d095dcf4fb6e4cc701db75c7a2c1076440d3df8\n",
      " is being fetched\n",
      "Page number with id Graph-Traversal-Edit-Distance-and-Extensions-Boroojeny-Shrestha/0d095dcf4fb6e4cc701db75c7a2c1076440d3df8\n",
      " is ready\n",
      "Page number with id DNA-methylation-regulates-discrimination-of-from-a-Sharifi-Zarchi-Gerovska/3056edcdfb86ec86bb8de369ce0f7fa0fae40e82\n",
      " is being fetched\n",
      "Page number with id DNA-methylation-regulates-discrimination-of-from-a-Sharifi-Zarchi-Gerovska/3056edcdfb86ec86bb8de369ce0f7fa0fae40e82\n",
      " is ready\n",
      "Page number with id Downregulation-of-Extracellular-Matrix-and-Cell-in-Hassani-Oryan/a3f82efd05a3f6dbced63d4450d64eae5a4ebb97 is being fetched\n",
      "Page number with id Downregulation-of-Extracellular-Matrix-and-Cell-in-Hassani-Oryan/a3f82efd05a3f6dbced63d4450d64eae5a4ebb97 is ready\n",
      "Page number with id c35f09bff4443d14f96ef69acb102091d8557396 is being fetched\n",
      "Page number with id c35f09bff4443d14f96ef69acb102091d8557396 is ready\n",
      "Page number with id 915c71dff20e9b0428a786e2fc008adf3c991a74 is being fetched\n",
      "Page number with id 915c71dff20e9b0428a786e2fc008adf3c991a74 is ready\n",
      "Page number with id 7fa779ef385b024a2c43f6c294eff23e9e0ae997 is being fetched\n",
      "Page number with id 7fa779ef385b024a2c43f6c294eff23e9e0ae997 is ready\n",
      "Page number with id 006cbdddc368936b0bf187e34b780ea6ca1c8835 is being fetched\n",
      "Page number with id 006cbdddc368936b0bf187e34b780ea6ca1c8835 is ready\n",
      "Page number with id 0f89491190727619c4ebc2231d7ccdcb3fc355ee is being fetched\n",
      "Page number with id 0f89491190727619c4ebc2231d7ccdcb3fc355ee is ready\n",
      "Page number with id 91ea87c521227c053a1798cf59125751726f6561 is being fetched\n",
      "Page number with id 91ea87c521227c053a1798cf59125751726f6561 is ready\n",
      "Page number with id 2f4cc59f2ca359f6396e3e84602853b23f660ef1 is being fetched\n",
      "Page number with id 2f4cc59f2ca359f6396e3e84602853b23f660ef1 is ready\n",
      "Page number with id 20a1daa0def6ad5495c952586e947f9d0cd64407 is being fetched\n",
      "Page number with id 20a1daa0def6ad5495c952586e947f9d0cd64407 is ready\n",
      "Page number with id 5095bdbf69aa1684fb4213e331b611a6463cb77c is being fetched\n",
      "Page number with id 5095bdbf69aa1684fb4213e331b611a6463cb77c is ready\n",
      "Page number with id aed94689bca9ce9c2aaca1f9693c301e1be007cc is being fetched\n",
      "Page number with id aed94689bca9ce9c2aaca1f9693c301e1be007cc is ready\n",
      "Page number with id da1db02f937bc01777a988a8a06055662ef28b28 is being fetched\n",
      "Page number with id da1db02f937bc01777a988a8a06055662ef28b28 is ready\n",
      "Page number with id ca59846bee048c397c520bd57748e81675b49993 is being fetched\n",
      "Page number with id ca59846bee048c397c520bd57748e81675b49993 is ready\n",
      "Page number with id 12614efd99ebad91ff0ce7bc42823f6808a36296 is being fetched\n",
      "Page number with id 12614efd99ebad91ff0ce7bc42823f6808a36296 is ready\n",
      "Page number with id 3db82f59404bf413a15ef2d9a92501c253bd8603 is being fetched\n",
      "Page number with id 3db82f59404bf413a15ef2d9a92501c253bd8603 is ready\n",
      "Page number with id d665cac9ab8060baaad672ea32b6049c0d5f9845 is being fetched\n",
      "Page number with id d665cac9ab8060baaad672ea32b6049c0d5f9845 is ready\n",
      "crawling done for this professor, writing info file\n",
      "Page number with id Transformer-based-deep-neural-network-language-for-Roshanzamir-Aghajan/6d8052588c62e5bf2a073ae414867a78784ff663\n",
      " is being fetched\n",
      "Page number with id Transformer-based-deep-neural-network-language-for-Roshanzamir-Aghajan/6d8052588c62e5bf2a073ae414867a78784ff663\n",
      " is ready\n",
      "Page number with id MG-BERT%3A-Multi-Graph-Augmented-BERT-for-Masked-BehnamGhader-Zakerinia/b9464b492f6638035d25b42f32ff3d51cb6d1e30\n",
      " is being fetched\n",
      "Page number with id MG-BERT%3A-Multi-Graph-Augmented-BERT-for-Masked-BehnamGhader-Zakerinia/b9464b492f6638035d25b42f32ff3d51cb6d1e30\n",
      " is ready\n",
      "Page number with id Deep-Learning-Based-Proarrhythmia-Analysis-Using-Golgooni-Mirsadeghi/b9e98f630e8eaf77ddcd0f80d1360b611ae61e70\n",
      " is being fetched\n",
      "Page number with id Deep-Learning-Based-Proarrhythmia-Analysis-Using-Golgooni-Mirsadeghi/b9e98f630e8eaf77ddcd0f80d1360b611ae61e70\n",
      " is ready\n",
      "Page number with id A-Deep-Learning-Framework-for-Viable-Tumor-Burden-Jahromi-Khani/1eae26fe1ca566f17468080c3aecab1c3f9efb66\n",
      " is being fetched\n",
      "Page number with id A-Deep-Learning-Framework-for-Viable-Tumor-Burden-Jahromi-Khani/1eae26fe1ca566f17468080c3aecab1c3f9efb66\n",
      " is ready\n",
      "Page number with id An-attribute-learning-method-for-zero-shot-Yazdanian-Shojaee/5a5f7d39433d68059e513b947a9fde62b5d4d3fe is being fetched\n",
      "Page number with id An-attribute-learning-method-for-zero-shot-Yazdanian-Shojaee/5a5f7d39433d68059e513b947a9fde62b5d4d3fe is ready\n",
      "Page number with id a7425f74a9e7bdd3ae6763c515c9534fb18a3560 is being fetched\n",
      "Page number with id a7425f74a9e7bdd3ae6763c515c9534fb18a3560 is ready\n",
      "Page number with id 73ee478f44296ee9a9e810fa106462ca52ece708 is being fetched\n",
      "Page number with id 73ee478f44296ee9a9e810fa106462ca52ece708 is ready\n",
      "Page number with id 9e7b0384bc48c3ae6dc1c66d6ee674902380a2c8 is being fetched\n",
      "Page number with id 9e7b0384bc48c3ae6dc1c66d6ee674902380a2c8 is ready\n",
      "Page number with id 9620bda083e3a3b7d55a5fdc5198c5f62052d323 is being fetched\n",
      "Page number with id 9620bda083e3a3b7d55a5fdc5198c5f62052d323 is ready\n",
      "Page number with id 4d4117e4e5214dcc887317e302db724df545729e is being fetched\n",
      "Page number with id 4d4117e4e5214dcc887317e302db724df545729e is ready\n",
      "Page number with id 365fab1146a72ad08799db52a07ef2a45038315e is being fetched\n",
      "Page number with id 365fab1146a72ad08799db52a07ef2a45038315e is ready\n",
      "Page number with id bdd4aee4ea351e615eae0e4ab317bf974f1f731b is being fetched\n",
      "Page number with id bdd4aee4ea351e615eae0e4ab317bf974f1f731b is ready\n",
      "Page number with id 8cd1d603498e65ae19baa59bdb31617f441d4296 is being fetched\n",
      "Page number with id 8cd1d603498e65ae19baa59bdb31617f441d4296 is ready\n",
      "Page number with id 360806c34ea0dcb5faab9824dababc094bb05c07 is being fetched\n",
      "Page number with id 360806c34ea0dcb5faab9824dababc094bb05c07 is ready\n",
      "Page number with id bbeae238e2d1373b75ce20ce96b4a5b87383a622 is being fetched\n",
      "Page number with id bbeae238e2d1373b75ce20ce96b4a5b87383a622 is ready\n",
      "Page number with id 5f994dc8cae24ca9d1ed629e517fcc652660ddde is being fetched\n",
      "Page number with id 5f994dc8cae24ca9d1ed629e517fcc652660ddde is ready\n",
      "Page number with id df2b0e26d0599ce3e70df8a9da02e51594e0e992 is being fetched\n",
      "Page number with id df2b0e26d0599ce3e70df8a9da02e51594e0e992 is ready\n",
      "Page number with id 31184789ef4c3084af930b1e0dede3215b4a9240 is being fetched\n",
      "Page number with id 31184789ef4c3084af930b1e0dede3215b4a9240 is ready\n",
      "Page number with id d6a13d8d168936a8947101d76fe060704d2f26ec is being fetched\n",
      "Page number with id d6a13d8d168936a8947101d76fe060704d2f26ec is ready\n",
      "Page number with id b36b2914f16c78b1bf88ee720342d893d8a9fc46 is being fetched\n",
      "Page number with id b36b2914f16c78b1bf88ee720342d893d8a9fc46 is ready\n",
      "crawling done for this professor, writing info file\n",
      "Page number with id Nucleus-segmentation-across-imaging-experiments%3A-Caicedo-Goodman/0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf\n",
      " is being fetched\n",
      "Page number with id Nucleus-segmentation-across-imaging-experiments%3A-Caicedo-Goodman/0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf\n",
      " is ready\n",
      "Page number with id Multiresolution-Knowledge-Distillation-for-Anomaly-Salehi-Sadjadi/6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d\n",
      " is being fetched\n",
      "Page number with id Multiresolution-Knowledge-Distillation-for-Anomaly-Salehi-Sadjadi/6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d\n",
      " is ready\n",
      "Page number with id Data-analysis-strategies-for-image-based-cell-Caicedo-Cooper/1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5\n",
      " is being fetched\n",
      "Page number with id Data-analysis-strategies-for-image-based-cell-Caicedo-Cooper/1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5\n",
      " is ready\n",
      "Page number with id Minimax-Optimal-Sparse-Signal-Recovery-With-Poisson-Rohban-Saligrama/0e16a10560b4f7d5f891b932c79d1b2005407acf\n",
      " is being fetched\n",
      "Page number with id Minimax-Optimal-Sparse-Signal-Recovery-With-Poisson-Rohban-Saligrama/0e16a10560b4f7d5f891b932c79d1b2005407acf\n",
      " is ready\n",
      "Page number with id ARAE%3A-Adversarially-Robust-Training-of-Autoencoders-Salehi-Arya/10b219619e88931fabb674037bbb633682775136 is being fetched\n",
      "Page number with id ARAE%3A-Adversarially-Robust-Training-of-Autoencoders-Salehi-Arya/10b219619e88931fabb674037bbb633682775136 is ready\n",
      "Page number with id dc90e115e348908891a4e990242e726b3d4e8b15 is being fetched\n",
      "Page number with id dc90e115e348908891a4e990242e726b3d4e8b15 is ready\n",
      "Page number with id 48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852 is being fetched\n",
      "Page number with id 48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852 is ready\n",
      "Page number with id 1294067d8237497b6cd3ee8731c3636215981de3 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id a1af04fa0a581a9f134a734363b1786ab2c355b7 is being fetched\n",
      "Page number with id a1af04fa0a581a9f134a734363b1786ab2c355b7 is ready\n",
      "Page number with id 779b489971775507fe6a39d98c52c4df56d9cce1 is being fetched\n",
      "Page number with id 779b489971775507fe6a39d98c52c4df56d9cce1 is ready\n",
      "Page number with id 31a27c6500b6652d7ecc055c9b08457ad90128c1 is being fetched\n",
      "Page number with id 31a27c6500b6652d7ecc055c9b08457ad90128c1 is ready\n",
      "Page number with id 713e881b5c3134debf934026edf6f0ba3cb42c3c is being fetched\n",
      "Page number with id 713e881b5c3134debf934026edf6f0ba3cb42c3c is ready\n",
      "Page number with id 0878ad6d3692c27eed44cc76b64662d228b4cf1e is being fetched\n",
      "Page number with id 0878ad6d3692c27eed44cc76b64662d228b4cf1e is ready\n",
      "Page number with id c89bfd998b0a6c656010b629814ab0cad3cff72e is being fetched\n",
      "Page number with id c89bfd998b0a6c656010b629814ab0cad3cff72e is ready\n",
      "Page number with id c26e4b47660692e500ad5afb5225e62b58b2e411 is being fetched\n",
      "Page number with id c26e4b47660692e500ad5afb5225e62b58b2e411 is ready\n",
      "Page number with id 4c1abd8969fc1c360f50373f6552bcfb3cc408b7 is being fetched\n",
      "Page number with id 4c1abd8969fc1c360f50373f6552bcfb3cc408b7 is ready\n",
      "Page number with id 5db790198b9acf4e5efe350acdd814238fcacaa7 is being fetched\n",
      "Page number with id 5db790198b9acf4e5efe350acdd814238fcacaa7 is ready\n",
      "Page number with id 0535625be630c6a67f4c244ebf3aa61ad088fc70 is being fetched\n",
      "Page number with id 0535625be630c6a67f4c244ebf3aa61ad088fc70 is ready\n",
      "Page number with id 3aa681914a7da79f7d7293f51a058eefe61c8bb7 is being fetched\n",
      "Page number with id 3aa681914a7da79f7d7293f51a058eefe61c8bb7 is ready\n",
      "Page number with id 41747cbdbed84762dfbfc305254c97021279dc6e is being fetched\n",
      "Page number with id 41747cbdbed84762dfbfc305254c97021279dc6e is ready\n",
      "Page number with id dbc7401e3e75c40d3c720e7db3c906d48bd742d7 is being fetched\n",
      "Page number with id dbc7401e3e75c40d3c720e7db3c906d48bd742d7 is ready\n",
      "crawling done for this professor, writing info file\n",
      "Page number with id Spatial-Aware-Dictionary-Learning-for-Hyperspectral-Soltani-Farani-Rabiee/5ca94050fcf3382b50ec44629c0dda80c8843558\n",
      " is being fetched\n",
      "Page number with id Spatial-Aware-Dictionary-Learning-for-Hyperspectral-Soltani-Farani-Rabiee/5ca94050fcf3382b50ec44629c0dda80c8843558\n",
      " is ready\n",
      "Page number with id Multiresolution-Knowledge-Distillation-for-Anomaly-Salehi-Sadjadi/6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d\n",
      " is being fetched\n",
      "Page number with id Multiresolution-Knowledge-Distillation-for-Anomaly-Salehi-Sadjadi/6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d\n",
      " is ready\n",
      "Page number with id A-Hybrid-Deep-Learning-Architecture-for-Mobile-Osia-Shamsabadi/6f0685d61328f0f90972fe822258d574b74e9c7a\n",
      " is being fetched\n",
      "Page number with id A-Hybrid-Deep-Learning-Architecture-for-Mobile-Osia-Shamsabadi/6f0685d61328f0f90972fe822258d574b74e9c7a\n",
      " is ready\n",
      "Page number with id Novel-dataset-for-fine-grained-abnormal-behavior-in-Rabiee-Haddadnia/c626a9d75dfd73e26cf30793d5ef71527cd9fa95\n",
      " is being fetched\n",
      "Page number with id Novel-dataset-for-fine-grained-abnormal-behavior-in-Rabiee-Haddadnia/c626a9d75dfd73e26cf30793d5ef71527cd9fa95\n",
      " is ready\n",
      "Page number with id Deep-Private-Feature-Extraction-Ossia-Taheri/e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da is being fetched\n",
      "Page number with id Deep-Private-Feature-Extraction-Ossia-Taheri/e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da is ready\n",
      "Page number with id 96d5444cfd4fb60ed7fd4460a92683fda358eb23 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id 498a42bd4e51182e5e7931a80dcfa2e0a007f33d is being fetched\n",
      "Page number with id 498a42bd4e51182e5e7931a80dcfa2e0a007f33d is ready\n",
      "Page number with id e30fcfa9e0e8c7d03315848e823781b7a72203ef is being fetched\n",
      "Page number with id e30fcfa9e0e8c7d03315848e823781b7a72203ef is ready\n",
      "Page number with id 67bcbb8ffd79c385edd734b9c9cf528ecc8ea343 is being fetched\n",
      "Page number with id 67bcbb8ffd79c385edd734b9c9cf528ecc8ea343 is ready\n",
      "Page number with id a87f0358993d9d24eb9f52f8534c60be9a213503 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id 0974a7957910de34c709e2d63411834f59323774 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id 692541a740b2b3c5c82a47390a7cbb40872efec5 is being fetched\n",
      "Page number with id 692541a740b2b3c5c82a47390a7cbb40872efec5 is ready\n",
      "Page number with id d5adb37bf7cb56774e6cdae87d04770f892ac754 is being fetched\n",
      "Page number with id d5adb37bf7cb56774e6cdae87d04770f892ac754 is ready\n",
      "Page number with id 28f3446718da6fed450bded33576322bd8d60448 is being fetched\n",
      "Page number with id 28f3446718da6fed450bded33576322bd8d60448 is ready\n",
      "Page number with id 0e8446c00ed21c19f62d71ab208a7b3601671766 is being fetched\n",
      "Page number with id 0e8446c00ed21c19f62d71ab208a7b3601671766 is ready\n",
      "Page number with id 3b3aefbbdb64e5812f133f220b3f129a36a30065 is being fetched\n",
      "Page number with id 3b3aefbbdb64e5812f133f220b3f129a36a30065 is ready\n",
      "Page number with id 2013f18832e0fb11c7aaf1f4f8da453aebda488c is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id 61840de4d9610558d510cfcf32986e93511a4cef is being fetched\n",
      "Page number with id 61840de4d9610558d510cfcf32986e93511a4cef is ready\n",
      "Page number with id 180300ff8282220c76c7c41ded4d9d8c1be4d3fc is being fetched\n",
      "Page number with id 180300ff8282220c76c7c41ded4d9d8c1be4d3fc is ready\n",
      "Page number with id ac62ed8a9b77d613189b63004f4a5d4c5cc082fe is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id f48d6b404932c23b701d01d7f7382384103b0bd6 is being fetched\n",
      "Page number with id f48d6b404932c23b701d01d7f7382384103b0bd6 is ready\n",
      "Page number with id 205a1c89058ea55fe536c6484a62213d1d0f0160 is being fetched\n",
      "Page number with id 205a1c89058ea55fe536c6484a62213d1d0f0160 is ready\n",
      "Page number with id 854c92e791ca2d81fec787736866b892b73b87f6 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id 61724a421569317ba470d48ebdd7316ab8e91b50 is being fetched\n",
      "Page number with id 61724a421569317ba470d48ebdd7316ab8e91b50 is ready\n",
      "Page number with id 50e111f895b3b059f927efb0cb917edeb3eadad2 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id 8dbe9e08808d54a3481262e8cd8597bd6a9eb975 is being fetched\n",
      "Page number with id 8dbe9e08808d54a3481262e8cd8597bd6a9eb975 is ready\n",
      "Page number with id 3369909da934c17cf86f90954e92bdda25b03be0 is being fetched\n",
      "Page number with id 3369909da934c17cf86f90954e92bdda25b03be0 is ready\n",
      "crawling done for this professor, writing info file\n",
      "Page number with id The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95\n",
      " is being fetched\n",
      "Page number with id The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95\n",
      " is ready\n",
      "Page number with id Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52\n",
      " is being fetched\n",
      "Page number with id Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52\n",
      " is ready\n",
      "Page number with id Event-Detection-and-Summarization-in-Soccer-Videos-Tavassolipour-Karimian/ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73\n",
      " is being fetched\n",
      "Page number with id Event-Detection-and-Summarization-in-Soccer-Videos-Tavassolipour-Karimian/ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73\n",
      " is ready\n",
      "Page number with id An-efficient-PCA-based-color-transfer-method-Abadpour-Kasaei/53fc0415e0d00f9691994a49b8232a1cc2dfad5f\n",
      " is being fetched\n",
      "Page number with id An-efficient-PCA-based-color-transfer-method-Abadpour-Kasaei/53fc0415e0d00f9691994a49b8232a1cc2dfad5f\n",
      " is ready\n",
      "Page number with id Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4 is being fetched\n",
      "Page number with id Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4 is ready\n",
      "Page number with id 786577081e00d69eeac8e9612eaf2dad59765e73 is being fetched\n",
      "Page number with id 786577081e00d69eeac8e9612eaf2dad59765e73 is ready\n",
      "Page number with id 219e9a4527110baf1feb3df20db12064eeafdfb7 is being fetched\n",
      "Page number with id 219e9a4527110baf1feb3df20db12064eeafdfb7 is ready\n",
      "Page number with id 350d507f5d899e4d7293b1aa951aa0f81b9fd30a is being fetched\n",
      "Page number with id 350d507f5d899e4d7293b1aa951aa0f81b9fd30a is ready\n",
      "Page number with id 047ea298464b041a90c4ab4e716356c019d613ab is being fetched\n",
      "Page number with id 047ea298464b041a90c4ab4e716356c019d613ab is ready\n",
      "Page number with id 966aad492f75b17f698e981e008b73b51816c6aa is being fetched\n",
      "Page number with id 966aad492f75b17f698e981e008b73b51816c6aa is ready\n",
      "Page number with id 4b1a47709d0546e5bc614bf9a521c550e6881d04 is being fetched\n",
      "Page number with id 4b1a47709d0546e5bc614bf9a521c550e6881d04 is ready\n",
      "Page number with id c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id dd45fe910a0200d43aaa77362f658542f6e175ff is being fetched\n",
      "Page number with id dd45fe910a0200d43aaa77362f658542f6e175ff is ready\n",
      "Page number with id ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7 is being fetched\n",
      "Page number with id ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7 is ready\n",
      "Page number with id 2415fc06de82ab41ad8b9615162247afb02974af is being fetched\n",
      "Page number with id 2415fc06de82ab41ad8b9615162247afb02974af is ready\n",
      "Page number with id 8133e66c9c03095ef605090e6a72b752dc774d92 is being fetched\n",
      "Page number with id 8133e66c9c03095ef605090e6a72b752dc774d92 is ready\n",
      "Page number with id f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46 is being fetched\n",
      "unknown Exception occurred\n",
      "Page number with id 301fee6989cc75f84343836cab5a25691d58dc5c is being fetched\n",
      "Page number with id 301fee6989cc75f84343836cab5a25691d58dc5c is ready\n",
      "Page number with id 8e75f635dd7578926aa7ae19ff29a8fc5c3911ae is being fetched\n",
      "Page number with id 8e75f635dd7578926aa7ae19ff29a8fc5c3911ae is ready\n",
      "Page number with id 156733fbf757d4e8a333537518c8961163c4fbf7 is being fetched\n",
      "Page number with id 156733fbf757d4e8a333537518c8961163c4fbf7 is ready\n",
      "Page number with id 6e69f7ea9f63b651ebd676d51d6cca1a483cb2ec is being fetched\n",
      "Page number with id 6e69f7ea9f63b651ebd676d51d6cca1a483cb2ec is ready\n",
      "Page number with id 4d7ef4f116a535750529e2853c181d5d3b678646 is being fetched\n",
      "Page number with id 4d7ef4f116a535750529e2853c181d5d3b678646 is ready\n",
      "crawling done for this professor, writing info file\n",
      "crawling done writing info file\n"
     ]
    }
   ],
   "source": [
    "list_of_output_addresses = ['crawled_paper_Sharifi.json', 'crawled_paper_Soleymani.json', 'crawled_paper_Rohban.json', 'crawled_paper_Rabiee.json', 'crawled_paper_Kasaei.json']\n",
    "data = crawl(list_of_output_addresses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>PageRank \n",
    "        شخصی‌سازی‌شده\n",
    "        (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش، الگوریتم \n",
    "    PageRank \n",
    "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم \n",
    "    PageRank\n",
    "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse.linalg as sla\n",
    "\n",
    "def calculate_page_rank(alpha, file_address='crawled_paper_Rohban.json', output_address='PageRank.json'):\n",
    "    with open(file_address, 'r') as f:\n",
    "        crawled_page = json.load(f)\n",
    "\n",
    "    matrix_row = {}\n",
    "    for i in range(0, len(crawled_page)):\n",
    "        matrix_row[crawled_page[i]['ID']] = i\n",
    "    paper_count = len(crawled_page)\n",
    "    P = np.full((paper_count, paper_count), alpha * (1 / paper_count), dtype=float)\n",
    "\n",
    "    for paper in crawled_page:\n",
    "        paper_id = paper['ID']\n",
    "        row = matrix_row[paper_id]\n",
    "        references = paper['References']\n",
    "        if references:\n",
    "\n",
    "            nodes = []\n",
    "            for reference in references:\n",
    "                if matrix_row.__contains__(reference):\n",
    "                    column = matrix_row[reference]\n",
    "                    nodes.append(column)\n",
    "\n",
    "            if len(nodes) != 0:\n",
    "                score = (1 / len(nodes)) * (1 - alpha)\n",
    "                for node in nodes:\n",
    "                    P[row][node] += score\n",
    "            else:\n",
    "                P[row] = (1 / paper_count)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    eval, evec = sla.eigs(P.T, k=1, which='LM')\n",
    "    u = (evec / evec.sum()).real\n",
    "    output = {}\n",
    "    for paper in crawled_page:\n",
    "        paper_id = paper['ID']\n",
    "        output[paper_id] = u[matrix_row[paper_id]][0]\n",
    "    sorted_output = dict(sorted(output.items(), key=lambda item: item[1], reverse=True))\n",
    "    with open(output_address, 'w') as f:\n",
    "        json.dump(sorted_output, f, indent=4)\n",
    "\n",
    "    f.close()\n",
    "    return sorted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'54e325aee6b2d476bbbb88615ac15e251c6e8214': 0.012260174045251527,\n",
       " '4474f9f90cd8300e7c7e28834bd094b68909b7ed': 0.012243914830577815,\n",
       " 'b4626f94678714c172cbaa273564afb68e9937cd': 0.012119569067291528,\n",
       " '8b73514fd708f6bdd3786d47a8673a97fdce4dfe': 0.010955849525951057,\n",
       " '5f5dc5b9a2ba710937e2c413b37b053cd673df02': 0.010685606927908972,\n",
       " '843959ffdccf31c6694d135fad07425924f785b1': 0.010369691988856192,\n",
       " 'e2b7f37cd97a7907b1b8a41138721ed06a0b76cd': 0.010086614138186449,\n",
       " 'abd1c342495432171beb7ca8fd9551ef13cbd0ff': 0.0094597054156077,\n",
       " '9d3f0d47449c7db37d1bae3b70db2928610a8db7': 0.009414066280413106,\n",
       " '4f9f7434f06cbe31e54a0bb118975340b9e0a4c9': 0.008529259370807063,\n",
       " 'b365b8e45b7d81f081de44ac8f9eadf9144f3ca5': 0.00739298529707917,\n",
       " 'a898ad13c96e5c068a2e4fc88227278e646b712e': 0.007246653117918011,\n",
       " 'bee044c8e8903fb67523c1f8c105ab4718600cdb': 0.0068822162998874795,\n",
       " '571b0750085ae3d939525e62af510ee2cee9d5ea': 0.006788325170605775,\n",
       " 'a2e75b7d9727b7ea7cefb4060d93ac75813e3c48': 0.006542664411377098,\n",
       " '195d0a8233a7a46329c742eaff56c276f847fadc': 0.006515107650438257,\n",
       " 'bfc8a8724b36cd2b1d068d1f997400e74791a68d': 0.006240923683252516,\n",
       " '2cf150bbc19e78ee3ea297cc447616922a1894c7': 0.006212724119525151,\n",
       " 'c1180048929ed490ab25e0e612f8f7c3d7196450': 0.005923693537494955,\n",
       " '3424286d6d39de51080ddd683646565545d015e2': 0.00567422687901764,\n",
       " 'b5e853572b2f3134acafa76d5ae80b9f28c7dca8': 0.005388930997355812,\n",
       " '317aee7fc081f2b137a85c4f20129007fd8e717e': 0.005257244516837742,\n",
       " '7aa39f7f3b69473705e247dd2b3a9689f10fbbc3': 0.005207614022901367,\n",
       " '8388f1be26329fa45e5807e968a641ce170ea078': 0.005059424172636049,\n",
       " '3e36828fb7c7d9239afb13273b2988381abeece6': 0.004983422628074425,\n",
       " '018300f5f0e679cee5241d9c69c8d88e00e8bf31': 0.004879051974542227,\n",
       " 'c8d4be093cd4da26f0402b6b9b2680d7ffae2959': 0.004696412249649784,\n",
       " '7d1096bc057b6e5e08a6ae821768d4f742d37b77': 0.0046289990762329375,\n",
       " 'd6f2f611da110b5b5061731be3fc4c7f45d8ee23': 0.004562340439877776,\n",
       " 'f35dfefdd5c44c5e717e17bb72c273b22b0878f1': 0.004441236099243348,\n",
       " '7733d1213c533c05cd77f2e9fe5c7410f6085f95': 0.004402103469474026,\n",
       " 'c87d573f82cfaaf7947476e56f9549dfe24ec896': 0.0042726298380189156,\n",
       " '915df1a8dda45221204f3ecbf70b07d8b34d7ba8': 0.004241636736421038,\n",
       " 'b959164d1efca4b73986ba5d21e664aadbbc0457': 0.004239490089973484,\n",
       " '819167ace2f0caae7745d2f25a803979be5fbfae': 0.004214981742197252,\n",
       " 'e163a2e89c136cb4442e34c72f7173a0ff46dc79': 0.004194529878906332,\n",
       " '6adf016e7531c91100d3cf4a74f5d4c87b26b528': 0.004182882207610379,\n",
       " 'a605a375d0803794adee9eac225011d294dfbada': 0.004111502022788203,\n",
       " '60fef33549f57f5cbb6712a510c3a444ab682429': 0.004055787386329323,\n",
       " '74b6fa545810f7953ca37ec9b765152d345a7081': 0.004047628654291003,\n",
       " '1eafe60d624180b892bf0d5c7b5d1f8b48dbfcb6': 0.004013682575108657,\n",
       " '144feb8f6b2bd36775ef6cc39452387c671314ff': 0.003931317756819242,\n",
       " '229f46e78c8ba96da2febc39a07e1c8e4daf4b7a': 0.003905297214124529,\n",
       " '8125727fe5b62492f4ccb1e25c66d473dd5c83c9': 0.0039039749133286845,\n",
       " '4dcdae25a5e33682953f0853ee4cf7ca93be58a9': 0.003812014291842038,\n",
       " 'a6f5c8d82f27bd68eb4cc40bc3d0b00fccc5d469': 0.003806258612286742,\n",
       " '8b9b060985fe05362cd350752489213a0d1e3dbd': 0.0038051864292547462,\n",
       " 'cdecac6e2f578cfc56140e00aaa74a78f864fea2': 0.0037927771464561595,\n",
       " 'cd989ec0c4b17b06921668b2030020ce51736f7c': 0.0037152076381460355,\n",
       " '2c03df8b48bf3fa39054345bafabfeff15bfd11d': 0.0036824552881225344,\n",
       " '040b4f257b8b048ffb0e61e5ee4a8d16ce2d32da': 0.003646146705424629,\n",
       " '68f30bd22817a17adc837eb285e51c9628f00e8d': 0.0035927696209561767,\n",
       " '268241fd604918a86e1b27e1a880d47e0ecc2d5b': 0.0035814332694142884,\n",
       " '5c5be36e3111e42247d78a6d529e4b1d7d2ced12': 0.0035735629112579144,\n",
       " 'ceca70071d3ff9f904d94dd971f6c56ce135662e': 0.0035689819368910102,\n",
       " 'b70151017955f66aee8cef6880f5f246e4ec39fb': 0.0035587555771487175,\n",
       " 'dc5b06753fac11268bc2300b7c25d50cbbcdeb5c': 0.0035254213760846473,\n",
       " '2e37e4844f214c34d0e4c75805dc35304603d736': 0.0034945204528432057,\n",
       " '039c4446c0bd231d1032fef9c221ce0d5ca2a01d': 0.003487328291954175,\n",
       " '5f56320c5979faeab78dbd9ddb7db755ba4550f3': 0.0034860572954084302,\n",
       " '6af440915b8a0718c93be1cf61905e41e620484a': 0.003484268876468753,\n",
       " '71d1ac92ad36b62a04f32ed75a10ad3259a7218d': 0.0034314711864698987,\n",
       " 'f076e4355c0facf111716dcab2837803367dd2d8': 0.003373613827603515,\n",
       " '36ab8bd64210ac5c4f7d326ed2c0a5745e91320f': 0.0033717468822045665,\n",
       " '1db6e3078597386ac4222ba6c3f4f61b61f53539': 0.0033419670197091405,\n",
       " '1b979ed50b3a7c68c79726d7c22e078c579501f3': 0.0033284258163801352,\n",
       " '7aa38b85fa8cba64d6a4010543f6695dbf5f1386': 0.003316989705281951,\n",
       " 'ae4c01a3463feb9d2af915e20f510d2749956f88': 0.0033115446960052452,\n",
       " '1972c73d96353e57599962fd6059572801382212': 0.0033085701761699894,\n",
       " '5d6906070d7e69ed794058720b52e46d2899e96b': 0.003301252758935167,\n",
       " 'c1317f812bfaec25958ec7b5b583eb035fe6d5a1': 0.0032810951731797678,\n",
       " 'c59c6afcfd5d03eb2c8fb5d1665afacc077771b9': 0.0032380491311306296,\n",
       " '1268de7bda769e651dc6c089d006c7edbe37f563': 0.0032375929443277263,\n",
       " '7d0effebfa4bed19b6ba41f3af5b7e5b6890de87': 0.003234293375168738,\n",
       " 'df40ce107a71b770c9d0354b78fdd8989da80d2f': 0.0032191479290515686,\n",
       " '50cc8dd825103c97204ff58a7c5da09c70bb128c': 0.003134234842524405,\n",
       " '136dee73f203df2f4831994bf4f0c0a4ad2e764e': 0.00312295685776218,\n",
       " '22f22e8ded8af32841e32f3c10be0679d60b9300': 0.0030563377296835676,\n",
       " 'a205ffddbd10b0a2be6953467a9d749aaa6c27d8': 0.0030318788106802404,\n",
       " '6c7500fd3f25946323420563329a6599e67de429': 0.003031878810680237,\n",
       " '3a320abd1b5444e9449c9098e0f3b7f2383b829e': 0.003026322822527783,\n",
       " 'd7c3c875f2f0c8ff1e8361802eca52c7b1d481c5': 0.0030072626931285608,\n",
       " 'a7138a6ca962b1a7774632e0bacf0ab911f71ee9': 0.0029915912638120764,\n",
       " '10a498003e9204f5fc1328e706510a37e514d8c7': 0.0029819024386029142,\n",
       " 'eb60fe884c53b420edbce57059b242cfcbae0f7c': 0.0029597521599905837,\n",
       " '8bedee52bf14e326d7fe2b1f1c1d0cb256aa53f6': 0.0029564723037264597,\n",
       " 'c7b7027659ffe67670d41a5a73b7eece8f0ff493': 0.0029227142716127807,\n",
       " '97e7c94a78ae17cfb90848c1cfca8c431082a7b2': 0.0029134147755376654,\n",
       " '547c854985629cfa9404a5ba8ca29367b5f8c25f': 0.0029089211498565014,\n",
       " '64993d9f5543631ce215dac860344c43533f0adf': 0.002896376612140834,\n",
       " '2b75ba7f75170b73d913c515cc0deefef6c88f5f': 0.0028758009359049976,\n",
       " 'f236a6e2f5a3363c63e9aaf4bde8cbe5d9c9f04a': 0.0028687414600477757,\n",
       " '99dc70afc02f49f56c0c511a578c10bc60c10c84': 0.0028368094853643334,\n",
       " '96c3209106cc0fd1310be658eac19124ec01bf85': 0.0028267295939886417,\n",
       " 'a346d207aa0fe69d1e01179746532ea0e71142c3': 0.0028081822268016332,\n",
       " 'f4cbe92def586e12b22f8e31109d1dd6ffa5e86d': 0.002784801290209221,\n",
       " '6c38f7dac2b0f4dc7d4297fb0bbbcc9fae8b5c46': 0.0027585634074777585,\n",
       " '8c0dbcc427b196f7ddef47293b2b651fdbfd59e7': 0.0027585634074777567,\n",
       " 'e2a85a6766b982ff7c8980e57ca6342d22493827': 0.0027316929538435887,\n",
       " '33a832399927971f144dee7fee50c0bcc5e1e659': 0.0027210126948440144,\n",
       " 'aedbddf72cb82c77d715a12dc43a98a0b1f1982f': 0.0027210126948440136,\n",
       " '732c21998e251d64cd58b6a86886ee5907efeaa5': 0.002712722030681563,\n",
       " '8a3b8f4dd0304861ab4d07e8f5befae03207e18e': 0.002709483635353649,\n",
       " '3bffa23a16c273ac2228a13e65dade6766ce7777': 0.0027020655258244847,\n",
       " 'ee07d8530c080caa5056a60bcc176569544a8927': 0.0027019299528522507,\n",
       " '48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852': 0.0026901829208457657,\n",
       " 'c051d469f9e9d5550d4633deecdb1ea8b11862dd': 0.0026841986363891914,\n",
       " 'd1f788e9ac58de5d2dc6a3928939b411cf9ec5a6': 0.002684198636389191,\n",
       " 'c14ae755c6e70d5f7ad257d09f9e119c9969a14d': 0.002630734443805265,\n",
       " 'fcf43325529c8b1cc26aeb52fd5d7e532abb0a40': 0.002610197087859253,\n",
       " '60a1a5b868e3a669b0e916506f3dda7636b2a4dd': 0.0025964865036696948,\n",
       " 'aae932cf9c2434f52b03991fcab050a61a960d48': 0.0025871182724776878,\n",
       " 'b66561622170d97f4127f4131d485faefe3833f7': 0.0025571101110056517,\n",
       " 'dbc7401e3e75c40d3c720e7db3c906d48bd742d7': 0.002556804792556023,\n",
       " '929a490198770abcb8c123d68a59384879b69adb': 0.0025440386268448514,\n",
       " '8d001fdf877f5de5d639811eab63c176231d1c22': 0.0025271509864769515,\n",
       " '927de77d0002851c150be9fb0ea2204880135f3f': 0.0025190636176866538,\n",
       " '88c5ee4435438fbc0d8e8e3039f5cf8ab80a135b': 0.002517688581258363,\n",
       " 'fdb0014af8197c8f303dca3fccad5d27bd264dcd': 0.002481147711653812,\n",
       " 'e01c7387bcc4526cdb758581ccca2627d5eac4a4': 0.0024623479561323713,\n",
       " '01e4e8d24b4ba213f14aaa5812ec3ea5c1863a03': 0.00246234795613237,\n",
       " '398ff09f1ef9fdc1d70f59eeb744f06c06998528': 0.002461292446624573,\n",
       " '67b9c2b376a01d8757dc6d704be450d1c46c4ced': 0.002458774300289013,\n",
       " 'c55585871acc109ce1dfe4d66a61d7d6baa7f58e': 0.0024560159617326937,\n",
       " '0cbc480e0d380bbaa04bfb21a396c9e8da6e930e': 0.002450111546006413,\n",
       " '8a2b9eac4e36e5f62a59f118c7bfc1ed9763b59c': 0.002448353951364786,\n",
       " '8381157eae4fbf8908d0312a9642f8e69e944449': 0.0024410586215501203,\n",
       " '599fd051c9438011ec5b581983c89e8922b4a5e6': 0.0024393029175805423,\n",
       " '39e0c341351f8f4a39ac890b96217c7f4bde5369': 0.0023886887262188354,\n",
       " '30d3cf944dc63c4451d393141641a205adf6030d': 0.002384613445921528,\n",
       " '0c6398c4231aa0f03121bee8632d5cba4da67de3': 0.002371668886859796,\n",
       " '8f7e961ef79dadbc484b5c9852678211aa379fc5': 0.002356739427423833,\n",
       " '4b23012689e0f17912fb38d4984775e567cff8d6': 0.0023499113310683937,\n",
       " 'a2017ec2c60d542af5e9993176ba68f89529dbce': 0.0023128639250848356,\n",
       " '966e3c7a65ec75a6359b55c0cecaf3896d318432': 0.002300614216279008,\n",
       " '00695a31a80221c7125e49885a4767896ec2c4f7': 0.0022961985129775074,\n",
       " '669832b732a20ccbdbb81c22393f4bc8f9371dbc': 0.0022877990160184076,\n",
       " '851ff5f13fbff7023717c3913f2df4a7551a374a': 0.0022789089992441476,\n",
       " '51ff6cfb98eec39da52035d87f329cf9e89462e6': 0.0022758061545546638,\n",
       " 'd97e70d8fa6cbe4fcac9096491ce6e6a5e974e88': 0.0022531838780903216,\n",
       " '29108acecb6da84e0d4833bc7c288c9feca56160': 0.002246760610441291,\n",
       " '8124c3f9bf7fb7cef9cb28ba77117f0923039b14': 0.0022394559397069967,\n",
       " '36748de338909976f72ffbadaf097470ec040da0': 0.0022329021243188006,\n",
       " 'af893d0ff0ebdde2e01ac5b893632070bf401ef2': 0.0022326207983637686,\n",
       " '90ce366042075b7956f5fd0bd44bafa3fbc8f27a': 0.002231286339866643,\n",
       " 'e2cfa3d7898d6eeac142ab9613f9bb2ea20683c3': 0.002225596824480254,\n",
       " 'e10d7c818b3fea4f37c762c241965b2f665c9b70': 0.0022189505442534183,\n",
       " 'e1b3ad532f9346d51578c32dc6b070c9744a8d88': 0.0022134716540206925,\n",
       " 'c486cec6473c6f97cc2037d475c99c00d6ff9b23': 0.0022121732399984126,\n",
       " '74d00a68bb3da55c9463928a7fd5a58cd5540788': 0.0021710779548664767,\n",
       " 'bb92178d5133f9165b704500cbe2e5a1b2dab01d': 0.002170345329547137,\n",
       " '2910bec6d4de87e22be5119cef3c488d2ae50e2a': 0.002167929404287806,\n",
       " '1ed22f981940b25d3ceac8846153b3521e8cae7b': 0.002164601645866238,\n",
       " 'd2fc13908e1d7ad9cebaed216e06ee73924bb274': 0.0021618995480485077,\n",
       " '69b9bf4b97798a20af27c5172290e0aa4c0a936f': 0.002158103098262609,\n",
       " 'e8b8a7778ace2a02f8db6fe321a54520c6b283ca': 0.002150759332633398,\n",
       " '061146b1d7938d7a8dae70e3531a00fceb3c78e8': 0.0021441853768388646,\n",
       " 'ece5045d0f495834b25d2188735bcee982edd8c2': 0.002135116276342463,\n",
       " 'a4cec122a08216fe8a3bc19b22e78fbaea096256': 0.0021272348009759566,\n",
       " '705fd4febe2fff810d2f72f48dcda20826eca77a': 0.0021251043523220245,\n",
       " '6edae8f3012ad49bd63f05a44fc11f86353d4c79': 0.002123213491064174,\n",
       " 'b90d44f59fcb74c71d3e31f67a3f09efab187a4e': 0.0021057449317288426,\n",
       " '9d5290fadb7625862a966e0330bd0f9e111fc99d': 0.0020995881880236027,\n",
       " '483813f8da20b4f5763eb12f80d210d9613cf44e': 0.002095214331906366,\n",
       " '6f68ce1e03c56c186256dac689a21f6405ae8d96': 0.0020921438126199495,\n",
       " '3262e77099cefe24cff1308f204e673cac832451': 0.0020903919267053553,\n",
       " '559a52d27ff8e3ae0cdf1e7948c137ff566285c8': 0.002086861511987186,\n",
       " 'b570697525b947451d8ba0933c9cbde5f13286b7': 0.0020858012074672066,\n",
       " 'cb83ea348192bff3acaff83d81cbcb0046f3205d': 0.002080293234686161,\n",
       " '353ecf7b66b3e9ff5e9f41145a147e899a2eea5c': 0.002076978286059746,\n",
       " '5214b65ed56efffd97493a59114a772dfb54caf1': 0.002076281398198226,\n",
       " '90f72fbbe5f0a29e627db28999e01a30a9655bc6': 0.002075916632366676,\n",
       " '567f01807f24abcc168c58b05b4c1659387f474f': 0.0020737690869148916,\n",
       " '779b489971775507fe6a39d98c52c4df56d9cce1': 0.0020737582850660573,\n",
       " '713e881b5c3134debf934026edf6f0ba3cb42c3c': 0.002073758285066055,\n",
       " '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35': 0.002070474064564061,\n",
       " 'fe87ea16d5eb1c7509da9a0314bbf4c7b0676506': 0.002070474064564061,\n",
       " 'eb053d8a7ae6a5fe4a0402faf36272af465cbff2': 0.0020562691855030278,\n",
       " 'a37473b030751378f4d66599d5d109c75ca71d01': 0.0020514509573172643,\n",
       " 'c8c04ed972d38e2326a53d322a6f2d7e0f8218c1': 0.002044353376272507,\n",
       " 'b4440c42d5ef6d0ae40f5646d1a6a1011333e7d3': 0.0020352471649528695,\n",
       " '5d666e2761bbac7d8e0ac724280d20fd24d71a6b': 0.002030409827766387,\n",
       " '1f4292905f62abd7c911b7d17f57575fed06f8c9': 0.002022084549397164,\n",
       " 'fc49ee43733ebd514f412186f1b992bc5278b3f6': 0.0020220845493971636,\n",
       " 'e990c93220a9c7de059c4faf36c4436b2c4bc36c': 0.002022084549397161,\n",
       " '29c736eb38861ecf346ce49eedf163c03974566b': 0.002020053154515216,\n",
       " '79a694782dad9f5fc2bbc2a21eab735586550ff5': 0.002017391022666653,\n",
       " '431ba9fae8fccad1665979d455c6307786e47318': 0.002016151293845624,\n",
       " 'fa757b408ec78abdd4487046c28fd55027a601e8': 0.0020133014441704004,\n",
       " 'a05b2552ba5dc6caea16ca709706d0ae3bbe4656': 0.0020133014441703965,\n",
       " 'c89bfd998b0a6c656010b629814ab0cad3cff72e': 0.0020132336174007964,\n",
       " '31b7afa0a6d0968382804e2e7a1470f11b6fb1ac': 0.0020081795163032256,\n",
       " '57d70f9d46363a8e9cadb5123eab4fda1a5081e0': 0.0019982144077641713,\n",
       " '7ba18c90be2166a848ab509f9e9de950750a91da': 0.0019970554898280785,\n",
       " '40521ea2dc363b5164e9bcdec9de908cab18b617': 0.0019970554898280767,\n",
       " 'd3c071dbbb4520ed5875f7e064a9da87240534db': 0.0019959093793404663,\n",
       " '05fd112b31b358135788c669a0f064d4e15f1ba7': 0.0019928511841411714,\n",
       " '357733cc76e31a499a27ba2da8612174aafb3213': 0.001982454792929857,\n",
       " 'e4d56ec1be5603ddb2cd5b9264e365a19dbd5217': 0.0019772169658353895,\n",
       " 'a295f76c2afb7f79a970ccf086f16168d976bb93': 0.0019705179995963247,\n",
       " '89b4fb65abb4a81bc492c3b686f5dfc5f175546d': 0.001970090881542442,\n",
       " 'c2a7df55c168a3d719eb85248da41734d7a8cf30': 0.001970065562511007,\n",
       " 'b58657eccdec07bd6b35ef00dab30670d1c9d5ed': 0.001970065562511006,\n",
       " '621a107cbc26866ae1c43b1aebcfc8a5157b555f': 0.001964221683412055,\n",
       " 'a6cb366736791bcccc5c8639de5a8f9636bf87e8': 0.0019625176644702653,\n",
       " '7ed849c45ee0d5e45423226e967bfaab4af35e72': 0.0019531563237958605,\n",
       " 'c6bb1291dc4d69d8362d05737dcd97020832c8d4': 0.0019504948243834017,\n",
       " '3aa681914a7da79f7d7293f51a058eefe61c8bb7': 0.0019475242982597808,\n",
       " '0535625be630c6a67f4c244ebf3aa61ad088fc70': 0.0019433088777007796,\n",
       " '82635fb63640ae95f90ee9bdc07832eb461ca881': 0.001928943601270629,\n",
       " '7502fb7b2690ebfba309f0dc195320f7e51de0af': 0.0019119943249767506,\n",
       " 'df775e0093d4e96e2690fa9f48f60d76e4fede19': 0.0019112209993450157,\n",
       " 'e399a626ba21fafb19b3661603ec9724058e951b': 0.0019053865985640883,\n",
       " '4a0ef8c1cd80b7f21f3b6fc3dc7a6654b3d923e3': 0.0018988193796210448,\n",
       " '318f82a3e593e391cfd0da7964b16d83299aa943': 0.0018968947710140227,\n",
       " '70f9968a356d840040a1c9207906f60376dc6bd4': 0.0018924417806606482,\n",
       " '6cf1d69e447e9687dbd2d92572f44bddbabd8192': 0.0018828878496883906,\n",
       " '9c24454b071bc8e96ea46c5064a7bddf07cca464': 0.0018828878496883895,\n",
       " 'aa0875ccc516862edc0b6bd2181ee27ce882933d': 0.0018779597871482123,\n",
       " 'cad075f54d629d5fbe5f1bf3b8941857ab625dd9': 0.001869617081261897,\n",
       " '713ccbb0d45c0c5955f687e38c2e4c850e0bfede': 0.0018696170812618966,\n",
       " 'f5d70adb117e69a61bb879434767e7ac4e2cda3a': 0.0018696170812618964,\n",
       " '169bba3c1055d8ccbc211f005d364e559d4fb44b': 0.0018696170812618953,\n",
       " '8725c05d1565ef28f6f02f14568cbfea4b4521dd': 0.0018696170812618948,\n",
       " 'dec6d5d1d76c60c337720e5629365f194a34a7d5': 0.0018676420786666599,\n",
       " '99dff291f260b3cc3ff190106b0c2e3e685223a4': 0.0018676420786666592,\n",
       " 'e716688ddc25dcc8871ef04c7f864063949aa8b9': 0.001863426658107656,\n",
       " 'f88cfc38dec02dcf050eb1f56d2d59d90b24e04c': 0.0018632158870797056,\n",
       " '28a0b9a6da483c184ea5b5b95de7f55ce47fdbf9': 0.001860471088371965,\n",
       " '1b225474e7a5794f98cdfbde8b12ccbc56799409': 0.0018603816674249802,\n",
       " '5db790198b9acf4e5efe350acdd814238fcacaa7': 0.0018590004665207016,\n",
       " 'fe09efb519b26d59c64c715c4efcbe752dc933be': 0.0018590004665207016,\n",
       " '5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d': 0.0018590004665207012,\n",
       " '3ecba59e99a4cd5f146270debc5ef8d214697c77': 0.0018590004665207,\n",
       " 'fd7789de401811fd8692466b8d49230e7184655f': 0.0018520177088656418,\n",
       " '1deb6bd9bc6c0112cd06348fd738d7f50ff4b907': 0.0018520177088656407,\n",
       " '8187b8e974fb8d3c39f6394651d389beedf5e2c8': 0.0018390422716518381,\n",
       " '0ac5d0c4563c416b6f79acdfbba75e12a7ada32d': 0.0018390422716518381,\n",
       " 'a3cb5678ad450d143fc1e806c069b27d86467348': 0.0018390422716518381,\n",
       " '175bb4c2c8691971321b7872456302a551a9abdd': 0.0018390422716518377,\n",
       " '46fb3d6d09709b0cf06ea07b68bec993d1d32133': 0.0018390422716518375,\n",
       " '536ce50e3834988a008e6b8ccd902c025f138444': 0.0018390422716518372,\n",
       " '12d812aa80691d016abda5303a6273b0ebeab7e0': 0.0018390422716518372,\n",
       " '908fbd1d905e2c55e29a7afd426f4faf1b706c5a': 0.001839042271651837,\n",
       " 'a43a8918b4ece9dba5ce5f24f1210915f256a4c1': 0.0018390422716518366,\n",
       " 'ab15a2d1ebab7c3190ad7b8d294d3737198dda12': 0.0018390422716518364,\n",
       " '80f8ef1da4845999d21c646ec0444c6a873a9f39': 0.0018344885766035316,\n",
       " '54e157572c0c1e5e585053231c83d62542f7e17e': 0.0018344885766035312,\n",
       " '91d43e2f2097574039fa52a75d1105d4f885d68a': 0.0018344885766035312,\n",
       " '224ef9f9ae9c367211a3d384aacec395242282f5': 0.001834488576603531,\n",
       " '7a55d9717b3e349f1e57bd03edbb6d6b48e7de11': 0.001834488576603531,\n",
       " '6ce2f215abdec16c9491b33cef38003cf84176b1': 0.0018344885766035301,\n",
       " '97602b63ea3fa4f43e396f1dda19ed48bf62d646': 0.0018339918387878659,\n",
       " 'fbb38946334941292800a82c99c0dc4feb0cb882': 0.0018299582703967946,\n",
       " '2c5ee8c30bba238fbcb31456b10ebb2cdb8d1a35': 0.0018299582703967946,\n",
       " '00af02c2cb48920af477115e870a42ac4f8a3834': 0.0018299582703967929,\n",
       " 'ec7e0ceea8f79735742ea671b3c37148cae9f22c': 0.0018299582703967926,\n",
       " '24a26e47c3577952f71264e97114c5b908636257': 0.0018206773696438322,\n",
       " '7dae942104dc8283504ce7a492c9ca12fa119189': 0.0018206773696438322,\n",
       " 'efd68f3724942c9de5dc804d3c7cb3f70f42234b': 0.0018206773696438315,\n",
       " '08dc94471605308669c8d3d8284ba94fcc93e345': 0.001820677369643831,\n",
       " '9f7a89bc9b8ebb7152acacc95a84daead92d8f2c': 0.0018206773696438304,\n",
       " '11e69f9757d3f4927a21a76dadc41eddaa35e2ec': 0.0018157781164181719,\n",
       " 'd8b8805b824a17471f6005c838806ba3c892dd08': 0.0018157781164181712,\n",
       " '1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5': 0.001815778116418171,\n",
       " '78bd4d517a6312ef2296243b5d6387c8f2e2abaf': 0.0018157781164181708,\n",
       " '60d4ec78a673119420bee41268672a8f8669bb31': 0.0018140084632293444,\n",
       " '3eeb8b5d5ee2db86fa359ec479b730a793d5971e': 0.001814008463229344,\n",
       " 'ca9f84c3922004ec6133aa9c2048ceeb17702fee': 0.0018140084632293438,\n",
       " '8080c7954944e4e293028768e7dcbaead190c85e': 0.0018140084632293433,\n",
       " '140b7fab48719b56e933216594eaa8b5fc361c1b': 0.0018133701549437862,\n",
       " 'ea53d8bd34060ed48ed15b9737a8f8a197e34baf': 0.0018126308403716603,\n",
       " '0bb38cc2e9c8799f9cf21fe8a0d17d66587dcf2f': 0.0018126308403716596,\n",
       " '83fe7496f9f823ad81083a5b3f326ab501eb3a34': 0.0018126308403716587,\n",
       " 'be414e62f977e82b56f6402d27c6a3144e318029': 0.0018126308403716587,\n",
       " '9e93ab699a70701f8100a7730b9f6ee250bbaa18': 0.001810218341619254,\n",
       " 'a1e06f125aeb3899c0eda9e9286820ccff76a494': 0.001810218341619254,\n",
       " '4cebfa50e9e48afc6260df161582f0271c84d86a': 0.0018102183416192539,\n",
       " 'a7fae7dba0db74cc21a7c7b70157fe601784b681': 0.0018082211546790496,\n",
       " '9eb1b16fbd4786eaac91f308d75609b9321868ce': 0.001808221154679048,\n",
       " '967d532a66dab7edcb818b0f9dc59fe8da7dc171': 0.0018081333694805705,\n",
       " '7b5be0cdec2a1b36cd8b61d161cff716b3594846': 0.001801377017216325,\n",
       " '7fc464470b441c691d10e7331b14a525bc79b8bb': 0.0018013770172163248,\n",
       " '8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8': 0.001801377017216324,\n",
       " '867bd62f7daeb6bb6f5ebe07af777c8378820ec4': 0.0018005581744024654,\n",
       " '7d4642da036febe5174da1390521444ef405c864': 0.0018005581744024635,\n",
       " '295d17c302c11e783defc08fd9c083320bba87f0': 0.0018005581744024632,\n",
       " '5e302195a3d4761e19fdaa8609e269f80820a07a': 0.0018005581744024628,\n",
       " '6cf98b123feac6504b0dc3a8b46e1462dd69121e': 0.0018005581744024622,\n",
       " '2586ebbb3d73dfbaca970e7cb22f7d011740cdfa': 0.001799958531329277,\n",
       " 'b5bfb2776c33f8d5de8b812da7a7d5ad56cb84e3': 0.0017974084883530337,\n",
       " '983571bce9c2439cdb8953406e066a4bd39e6831': 0.001797408488353033,\n",
       " '9f7ed95dcdeb8f9337d957579db604e10c87210e': 0.001797408488353033,\n",
       " 'f312852b31a76bc09b59aca6e4017a9382f9cb39': 0.0017943983058948565,\n",
       " '6f14a338e8837fae059cab41064155cd84cb9cd5': 0.0017943983058948552,\n",
       " '0f71f070ea61fd51a50e21e73c35f57b00ea7550': 0.0017943983058948546,\n",
       " 'ebf91683698038fb0d991c6ffd6f675ae5400990': 0.0017871708813273044,\n",
       " 'a878e1a752403b4cfb425c9b4d9459b9eb2b2cfe': 0.0017871708813273033,\n",
       " 'ff7bcaa4556cb13fc7bf03e477172493546172cd': 0.001785048458936203,\n",
       " '8acbe90d5b852dadea7810345451a99608ee54c7': 0.0017850484589362024,\n",
       " '62adfea3cc1cd9eb6b53e0e8a40be5dfda2adf8d': 0.0017850484589362024,\n",
       " 'b2d952fbd6951cbed68ea13003a045300970731a': 0.001785048458936202,\n",
       " 'ea99a5535388196d0d44be5b4d7dd02029a43bb2': 0.001785048458936202,\n",
       " 'db869fa192a3222ae4f2d766674a378e47013b1b': 0.001785048458936202,\n",
       " '27ccb11368a1198ce5b7622a883fe8a3f7d8053c': 0.0017850484589362018,\n",
       " 'fa39a47bc6bf818fca48c78fafc0aab1f0dc357f': 0.0017850484589362013,\n",
       " 'da6057368920585bcf2443295b98418840f1fc80': 0.001785048458936201,\n",
       " 'b0c065cd43aa7280e766b5dcbcc7e26abce59330': 0.0017850484589362005,\n",
       " 'b1b3ae440512f6fa3bef48938a7b74a7ba873de9': 0.001785029071893313,\n",
       " '95df3ed3f0be8092166385467b4c60bce1b7673f': 0.0017850290718933125,\n",
       " '36b9be6122cefacf7a77d1c147a80176916a5711': 0.0017845280366449666,\n",
       " '1a55ae68b459888c2ea8e0da5c1d35aea41623dc': 0.0017845280366449664,\n",
       " 'f203242b7c0e953d2d98c495ccc602e456e123cd': 0.0017845280366449658,\n",
       " 'b3acb6f183b5f4b651f53c0eec5cb5c805224ac1': 0.0017833336674865814,\n",
       " '1130d8fdd931225c2d7563c3808367726cfa1c3a': 0.0017807903126345736,\n",
       " 'e9bc178f3dc147be47e34c27667c5484ba6d5d40': 0.0017800662129589548,\n",
       " '31a27c6500b6652d7ecc055c9b08457ad90128c1': 0.0017798442360238518,\n",
       " 'c26e4b47660692e500ad5afb5225e62b58b2e411': 0.0017798442360238518,\n",
       " 'a1af04fa0a581a9f134a734363b1786ab2c355b7': 0.0017798442360238502,\n",
       " '0878ad6d3692c27eed44cc76b64662d228b4cf1e': 0.0017798442360238498,\n",
       " 'dc90e115e348908891a4e990242e726b3d4e8b15': 0.001779844236023849,\n",
       " '46f288017c752b2afe23d8accf44898b7427d46b': 0.0017793395565069254,\n",
       " '3447d8b47a8cf7ce9f04ede314f0ded8172fa470': 0.0017791182469275792,\n",
       " '2f9d50dbbb050abb68881cbffd5348892e0c315b': 0.0017791182469275788,\n",
       " '792250ae660b7c25f85eeea7dcae623e4301d97c': 0.0017791182469275788,\n",
       " 'c72e91b2e5c103f529510fa15a156d61c5633da1': 0.0017791182469275777,\n",
       " 'e5c33c8df40dd55cb3068b759a30f76bbd8b9933': 0.0017769571294224522,\n",
       " '8a7ce466e8f119f033aeace4210cc348ed62520b': 0.0017769571294224517,\n",
       " '8db554d7e597000f5a0e13712ef0cb3299c05187': 0.001776957129422451,\n",
       " '7cb2f4f7b65ae227872e72d6cfcdbf47714bd9a0': 0.0017751604354027365,\n",
       " 'bf3a23566fad67f46f2154455b1bd802df612984': 0.001775160435402736,\n",
       " '96a3a3bff71b9d470dfa02d67ce981cb42003133': 0.0017751604354027354,\n",
       " '4c8e87d996826725192d15273c14bc0f16a6f17e': 0.0017751604354027352,\n",
       " '908dfcd908fe3fef9486bff01dbc04ccbde556d0': 0.0017751604354027352,\n",
       " 'cf933679e4d3ba4e18e3c2ef13b3fe7b461fade2': 0.0017750160800726663,\n",
       " '4094a590c0ec21c9c0eefb636f793523dc9d5771': 0.0017750160800726663,\n",
       " '0dd9d8bdd445db5928428b0059820199d0c53331': 0.0017750160800726663,\n",
       " 'eb3fd030883947c22305617308be8a04bacc5514': 0.001775016080072666,\n",
       " 'b3488cfbe8dcd1f38f3ef1687cdebe821757f2ed': 0.001775016080072666,\n",
       " '6a6cd60c103ecfb989b63e05e65f689008592e39': 0.001775016080072666,\n",
       " '614b6456b0aeb024fee659b094bdd999cb7d021e': 0.001775016080072666,\n",
       " '83f4b74c4e2a247dc2e57e44ec1caeafa10e32e0': 0.001775016080072666,\n",
       " '2551a9e46238c4a912d2110674e5b644b2a2fb27': 0.0017750160800726659,\n",
       " 'd7c5d286ac2b41eb01c5961c04c1246da22d3702': 0.0017750160800726654,\n",
       " 'bd3f273590a34e121ddf5c2669c00750c8792e2b': 0.0017750160800726654,\n",
       " '68818b79d360aa92e479864e12414dc502820e1e': 0.0017750160800726654,\n",
       " '317c172f314f8cb634f7569ed5bf3ae7dd25c313': 0.0017746920553406251,\n",
       " 'fef6f1e04fa64f2f26ac9f01cd143dd19e549790': 0.0017746920553406251,\n",
       " '818c52f4ba56cb8cf152ad614f2f4803057a5cfe': 0.0017746920553406251,\n",
       " 'f5a951b9596be0df5ad7ede180b405c9e97a65c9': 0.0017746920553406245,\n",
       " 'a2e667e4382aaa8e02a17d0522c1a910790ab65b': 0.0017746920553406245,\n",
       " '68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6': 0.0017746920553406245,\n",
       " 'dbaeea209717db86bfcf359005d0130eff0b09c5': 0.0017746920553406245,\n",
       " 'ca42e4d7021d4e563bbeae7db35c1ce09fe38bfa': 0.0017746920553406243,\n",
       " 'bbd0e204f48a45735e1065c8b90b298077b73192': 0.0017746920553406243,\n",
       " 'f24dc0f3c1cc1da6c4fd12e1a65f55a6d82bb8ed': 0.001774692055340624,\n",
       " '8dce99e33c6fceb3e79023f5894fdbe733c91e92': 0.001774692055340624,\n",
       " 'ca4edb65a0664804e4819c5c809d0dfba9bdb2df': 0.0017746920553406238,\n",
       " 'fde52ab74c420dcbc0172a979eeeb4c9d36f4e4d': 0.0017746920553406238,\n",
       " '47eb8d7ea4f7c209040ddd82e264edf3945df6cb': 0.0017746920553406236,\n",
       " 'afac99b10a5c7e531f73e4a4866d6ee3c9e86cd4': 0.0017746920553406234,\n",
       " '06fad023ef0274e7d6727ecbd1ef46887a6806df': 0.0017746920553406232,\n",
       " '046d550f95db0dae9aa26b34c31cb502b5b72983': 0.0017704766347816216,\n",
       " 'd9d7ab13ce305ccee309c989a2341d72b1252070': 0.0017704766347816214,\n",
       " 'd65eb30e5f0d2013fd5e4f45d1413bc2969ee803': 0.0017704766347816205,\n",
       " '41747cbdbed84762dfbfc305254c97021279dc6e': 0.0017704766347816203,\n",
       " '4c1abd8969fc1c360f50373f6552bcfb3cc408b7': 0.00177047663478162,\n",
       " '46dfa751eade5e60ab16b51496a9c764bced2322': 0.0017704766347816198,\n",
       " '8f4f9faa26027f8eae4474d90f6d31c0749acd49': 0.0017704766347816198,\n",
       " '0314e777333a63aca5735ea136c74e113aa8801d': 0.0017704766347816192,\n",
       " '10b219619e88931fabb674037bbb633682775136': 0.0016861682236015438,\n",
       " '0e16a10560b4f7d5f891b932c79d1b2005407acf': 0.0016861682236015436,\n",
       " '0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf': 0.0016861682236015431,\n",
       " '6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d': 0.0016861682236015431}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = calculate_page_rank('0.5')\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش از الگوریتم \n",
    "PageRank\n",
    "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
    "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد \n",
    "خاص استفاده می‌کنیم. این تابع، یک \n",
    "    field \n",
    "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
    "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('54e325aee6b2d476bbbb88615ac15e251c6e8214', 0.012260174045251504),\n",
       " ('4474f9f90cd8300e7c7e28834bd094b68909b7ed', 0.012243914830577808),\n",
       " ('b4626f94678714c172cbaa273564afb68e9937cd', 0.012119569067291515),\n",
       " ('8b73514fd708f6bdd3786d47a8673a97fdce4dfe', 0.010955849525951059),\n",
       " ('5f5dc5b9a2ba710937e2c413b37b053cd673df02', 0.010685606927908955),\n",
       " ('843959ffdccf31c6694d135fad07425924f785b1', 0.010369691988856206),\n",
       " ('e2b7f37cd97a7907b1b8a41138721ed06a0b76cd', 0.010086614138186473),\n",
       " ('abd1c342495432171beb7ca8fd9551ef13cbd0ff', 0.009459705415607696),\n",
       " ('9d3f0d47449c7db37d1bae3b70db2928610a8db7', 0.009414066280413105),\n",
       " ('4f9f7434f06cbe31e54a0bb118975340b9e0a4c9', 0.00852925937080706)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def important_articles(Professor: str):\n",
    "    \"\"\"\n",
    "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
    "\n",
    "    Parameters:\n",
    "    Professor (str): Professor's name.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
    "    \"\"\"\n",
    "    file_name = 'crawled_paper_' + Professor + '.json'\n",
    "\n",
    "    try:\n",
    "         \n",
    "\n",
    "      result = sorted(calculate_page_rank(alpha=0.5, file_address=file_name).items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "      return result\n",
    "\n",
    "    except:\n",
    "      pass\n",
    "     \n",
    "\n",
    "important_articles('Rohban')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohammadali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mohammadali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_data(text : str):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    words = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in tokens:\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation+'1234567890()'))\n",
    "        if word == '':\n",
    "            continue\n",
    "        words.append(stemmer.stem(lemmatizer.lemmatize(word)))\n",
    "    return words\n",
    "corpus = []\n",
    "\n",
    "\n",
    "positional_indices = {}\n",
    "def read_file(address='crawled_paper_Rohban.json'):\n",
    "    with open(address, 'r') as f:\n",
    "        crawled_page = json.load(f)\n",
    "        \n",
    "        for paper in crawled_page:\n",
    "            id = paper[\"ID\"]\n",
    "            title_tokens = clean_data(paper[\"Title\"])\n",
    "            if paper[\"Abstract\"]:\n",
    "                abstract_tokens = clean_data(paper[\"Abstract\"])\n",
    "            else:\n",
    "                abstract_tokens = 'Nothing'\n",
    "            corpus.append({\"id\": id,\"title_token\": title_tokens, \"abs_token\": abstract_tokens})\n",
    "\n",
    "read_file()\n",
    "\n",
    "\n",
    "def get_posting_for_one_doc(tokens):\n",
    "    posting = {}\n",
    "    position = 0\n",
    "    for token in tokens:\n",
    "        if posting.__contains__(token):\n",
    "            posting.get(token).append(position)\n",
    "        else:\n",
    "            posting[token] = [position]\n",
    "        position += 1\n",
    "    return posting\n",
    "\n",
    "\n",
    "def construct_positional_indexes(corpus : list):\n",
    "    for item in corpus:\n",
    "\n",
    "        for word, posting in get_posting_for_one_doc(item[\"abs_token\"]).items():\n",
    "            if not positional_indices.__contains__(word):\n",
    "                positional_indices[word] = [[], []]\n",
    "            positional_indices.get(word)[1].append([item[\"id\"], posting])\n",
    "\n",
    "        for word, posting in get_posting_for_one_doc(item[\"title_token\"]).items():\n",
    "            if not positional_indices.__contains__(word):\n",
    "                positional_indices[word] = [[], []]\n",
    "            positional_indices.get(word)[0].append([item[\"id\"], posting])\n",
    "    return positional_indices\n",
    "\n",
    "positional_indices = construct_positional_indexes(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posting_list(word : str):\n",
    "    output_dict = {}\n",
    "    list_of_posting_lists = positional_indices[word]\n",
    "    for item in list_of_posting_lists:     \n",
    "        for ordered_tuple in item:\n",
    "            output_dict[ordered_tuple[0]] = ordered_tuple[1]\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def tokenize(list_word):\n",
    "    tokenize_list = {}\n",
    "    for word in list_word:\n",
    "        if tokenize_list.__contains__(word):\n",
    "            tokenize_list[word] += 1\n",
    "        else:\n",
    "            tokenize_list[word] = 1\n",
    "    return tokenize_list\n",
    "\n",
    "def strID_to_intID(rows):\n",
    "    dictionary = {}\n",
    "    counter = 0\n",
    "    for row_id in range(len(rows)):\n",
    "        dictionary[rows[row_id][0]] = counter + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def get_posting_for_doc(word,doc_id):\n",
    "    l = positional_indices.get(word)[1]\n",
    "    for posting in l:\n",
    "        if posting[0] == doc_id:\n",
    "            return posting\n",
    "        elif posting[0] > doc_id:\n",
    "            return None\n",
    "        \n",
    "\n",
    "def construct_full_corpus():\n",
    "    full_corpus = {}\n",
    "    with open('crawled_paper_Rohban.json', 'r') as f:\n",
    "        crawled_page = json.load(f)\n",
    "        for paper in crawled_page:\n",
    "            if paper[\"Abstract\"]:\n",
    "                full_corpus[paper[\"ID\"]] = {\"title\": paper[\"Title\"], \"abstract\": paper[\"Abstract\"]}\n",
    "            else:\n",
    "                full_corpus[paper[\"ID\"]] = {\"title\": paper[\"Title\"], \"abstract\": \"Nothing\"}\n",
    "\n",
    "    return full_corpus\n",
    "\n",
    "\n",
    "def cosine_normalization(w):\n",
    "    w = np.ones(w.shape)\n",
    "    tmp = np.sqrt(np.sum(w))\n",
    "    return np.sqrt(np.sum(w)) / tmp\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10 as log\n",
    "from collections import Counter\n",
    "def ltn_lnn (preferred_field, title_query: str, abstract_query: str, max_result_count: int = 10, weight: float = 0.5, personalization_weight = 0.5):\n",
    "    no_title, no_abstract = False, False\n",
    "    if title_query != \"\":\n",
    "        pass  #spelling correction\n",
    "    else:\n",
    "        no_title = True\n",
    "\n",
    "    if abstract_query != \"\":\n",
    "        pass    #spelling correction\n",
    "    else:\n",
    "        no_abstract = True\n",
    "\n",
    "    title_tokens = tokenize(clean_data(title_query))\n",
    "    abs_tokens = tokenize(clean_data(abstract_query))\n",
    "    prefrence_tokens = tokenize(clean_data(preferred_field))\n",
    "    for key in prefrence_tokens.keys():\n",
    "        item = key\n",
    "    \n",
    "    score =  {}\n",
    "\n",
    "\n",
    "\n",
    "    if not no_title:\n",
    "        for token in title_tokens:\n",
    "            if positional_indices.__contains__(token):\n",
    "                term_frequency = len(positional_indices.get(token)[0])\n",
    "                preference_freq = len(positional_indices.get(item)[0])\n",
    "                idf = 0\n",
    "                if term_frequency!=0:\n",
    "                    idf = log(6000 / term_frequency)\n",
    "                term_query_frequency = title_tokens.get(token)\n",
    "                for posting in positional_indices.get(token)[0]:\n",
    "                    doc_id = posting[0]\n",
    "                    term_doc_frequency = len(posting[1])\n",
    "                    if score.__contains__(doc_id):\n",
    "                        score[doc_id] += ((1 - personalization_weight) * (1 + log(term_doc_frequency)) * (1 + log(term_query_frequency)) * idf * weight) + personalization_weight * preference_freq\n",
    "                    else:\n",
    "                        score[doc_id] = ((1 - personalization_weight) * (1 + log(term_doc_frequency)) * (1 + log(term_query_frequency)) * idf * weight) + personalization_weight * preference_freq\n",
    "    max_idf = {}\n",
    "    \n",
    "    if not no_abstract:\n",
    "        for token in abs_tokens:\n",
    "            if positional_indices.__contains__(token):\n",
    "                term_frequency = len(positional_indices.get(token)[1])\n",
    "                preference_freq = len(positional_indices.get(item)[0])\n",
    "\n",
    "                idf = 0\n",
    "                if term_frequency !=0:\n",
    "                    idf = log(6000 / term_frequency)\n",
    "                max_idf[token] = idf\n",
    "                for posting in positional_indices.get(token)[1]:\n",
    "                    doc_id = posting[0]\n",
    "                    term_doc_frequency = len(posting[1])\n",
    "                    term_query_frequency = abs_tokens.get(token)\n",
    "                    if score.__contains__(doc_id):\n",
    "                        score[doc_id] += ((1 - personalization_weight) * (1 + log(term_doc_frequency)) * (1 + log(term_query_frequency)) * idf * (1 - weight) + personalization_weight * preference_freq)\n",
    "                    else:\n",
    "                        score[doc_id] = ((1 - personalization_weight) * (1 + log(term_doc_frequency)) * (1 + log(term_query_frequency)) * idf * (1 - weight) + personalization_weight * preference_freq)\n",
    "\n",
    "    c = Counter(score)\n",
    "    result = c.most_common(max_result_count)\n",
    "    highlighted_result = []\n",
    "\n",
    "    full_corpus = construct_full_corpus()\n",
    "    for doc_id, score in result:\n",
    "        doc = full_corpus[doc_id]\n",
    "        title = doc[\"title\"]\n",
    "\n",
    "        snippet = \" \".join(doc[\"abstract\"].split(\" \")[0: 16])\n",
    "\n",
    "        important_words = []\n",
    "        for word in (abstract_query + title_query):\n",
    "            if word in doc[\"abstract\"]:\n",
    "                important_words.append(word)\n",
    "        \n",
    "        snippet += \"...\" + \"\".join(important_words)\n",
    "        \n",
    "        \n",
    "\n",
    "        highlighted_result.append([doc_id,title,snippet+'...'])\n",
    "    return highlighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltc_lnc(preferred_field, title_query: str, abstract_query: str, max_result_count: int = 10, weight: float = 0.5, personalization_weight = 0.5):\n",
    "\n",
    "    no_title, no_abstract = False, False\n",
    "    if title_query != \"\":\n",
    "        pass  #spelling correction\n",
    "    else:\n",
    "        no_title = True\n",
    "\n",
    "    if abstract_query != \"\":\n",
    "        pass    #spelling correction\n",
    "    else:\n",
    "        no_abstract = True\n",
    "\n",
    "    title_tokens = tokenize(clean_data(title_query))\n",
    "    abs_tokens = tokenize(clean_data(abstract_query))\n",
    "    prefrence_tokens = tokenize(clean_data(preferred_field))\n",
    "    for key in prefrence_tokens.keys():\n",
    "        item = key\n",
    "    score =  {}\n",
    "    if not no_title:\n",
    "        for token in title_tokens:\n",
    "            if positional_indices.__contains__(token):\n",
    "                term_frequency = len(positional_indices.get(token)[0])\n",
    "                preference_freq = len(positional_indices.get(item)[0])\n",
    "\n",
    "                vec = np.ones(shape=term_frequency)\n",
    "                idf = 0\n",
    "                if term_frequency!=0:\n",
    "                    idf = log(6000 / term_frequency)\n",
    "                term_query_frequency = title_tokens.get(token)\n",
    "                for posting in positional_indices.get(token)[0]:\n",
    "                    doc_id = posting[0]\n",
    "                    term_doc_frequency = len(posting[1])\n",
    "                    if score.__contains__(doc_id):\n",
    "                        score[doc_id] += ((1 - personalization_weight)* ((1+log(term_doc_frequency))*(1+log(term_query_frequency))*idf*weight / cosine_normalization(vec)) + personalization_weight * preference_freq)\n",
    "                    else:\n",
    "                        score[doc_id] = ((1 - personalization_weight)* ((1+log(term_doc_frequency))*(1+log(term_query_frequency))*idf*weight / cosine_normalization(vec)) + personalization_weight * preference_freq)\n",
    "    max_idf = {}\n",
    "    if not no_abstract:\n",
    "        for token in abs_tokens:\n",
    "            if positional_indices.__contains__(token):\n",
    "                term_frequency = len(positional_indices.get(token)[1])\n",
    "                preference_freq = len(positional_indices.get(item)[0])\n",
    "\n",
    "                idf =0\n",
    "                if term_frequency !=0:\n",
    "                    idf = log(6000 / term_frequency)\n",
    "                max_idf[token] = idf\n",
    "                for posting in positional_indices.get(token)[1]:\n",
    "                    doc_id = posting[0]\n",
    "                    term_doc_frequency = len(posting[1])\n",
    "                    term_query_frequency = abs_tokens.get(token)\n",
    "                    if score.__contains__(doc_id):\n",
    "                        score[doc_id] += ((1 - personalization_weight) * (1+log(term_doc_frequency))*(1+log(term_doc_frequency))*(1+log(term_query_frequency))*idf*(1 - weight) + personalization_weight * preference_freq)\n",
    "                    else:\n",
    "                        score[doc_id] = ((1 - personalization_weight) * (1+log(term_doc_frequency))*(1+log(term_doc_frequency))*(1+log(term_query_frequency))*idf*(1 - weight) + personalization_weight * preference_freq)\n",
    "    c = Counter(score)\n",
    "    result = c.most_common(max_result_count)\n",
    "    highlighted_result = []\n",
    "\n",
    "    \n",
    "    \n",
    "    full_corpus = construct_full_corpus()\n",
    "    for doc_id, score in result:\n",
    "        doc = full_corpus[doc_id]\n",
    "        title = doc[\"title\"]\n",
    "\n",
    "        snippet = \" \".join(doc[\"abstract\"].split(\" \")[0: 16])\n",
    "\n",
    "        important_words = []\n",
    "        for word in (abstract_query + title_query):\n",
    "            if word in doc[\"abstract\"]:\n",
    "                important_words.append(word)\n",
    "        \n",
    "        snippet += \"...\" + \"\".join(important_words)\n",
    "        \n",
    "        \n",
    "\n",
    "        highlighted_result.append([doc_id,title,snippet+'...'])\n",
    "\n",
    "    return highlighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_doc_collections():\n",
    "    doc_collection = {}\n",
    "    with open('crawled_paper_Rohban.json', 'r') as f:\n",
    "        crawled_page = json.load(f)\n",
    "        for paper in crawled_page:\n",
    "            if paper[\"Abstract\"]:\n",
    "                doc_collection[paper[\"ID\"]] = {\"id\": paper[\"ID\"], \"title\": paper[\"Titile\"], \"abstract\": paper[\"Abstract\"]}\n",
    "            else:\n",
    "                doc_collection[paper[\"ID\"]] = {\"id\": paper[\"ID\"], \"title\": paper[\"Titile\"], \"abstract\": \"Nothing\"}\n",
    "    return doc_collection\n",
    "    \n",
    "\n",
    "def BM25(preferred_field, title_query: str, abstract_query: str, max_result_count: int = 10, weight: float = 0.5, personalization_weight = 0.5):\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    all_docs = prepare_doc_collections()\n",
    "    avg_doc_len = sum(len(doc) for doc in all_docs.values()) / len(all_docs)\n",
    "\n",
    "    no_title, no_abstract = False, False\n",
    "    if title_query != \"\":\n",
    "        pass  #spelling correction\n",
    "    else:\n",
    "        no_title = True\n",
    "\n",
    "    if abstract_query != \"\":\n",
    "        pass   #spelling correction\n",
    "    else:\n",
    "        no_abstract = True\n",
    "        \n",
    "    title_tokens = tokenize(clean_data(title_query))\n",
    "    abs_tokens = tokenize(clean_data(abstract_query))\n",
    "    prefrence_tokens = tokenize(clean_data(preferred_field))\n",
    "    for key in prefrence_tokens.keys():\n",
    "        item = key\n",
    "        \n",
    "    score =  {}\n",
    "    all_docs = prepare_doc_collections()\n",
    "    if not no_title:\n",
    "        for token in title_tokens:\n",
    "            if positional_indices.__contains__(token):\n",
    "                term_frequency = len(positional_indices.get(token)[0])\n",
    "                preference_freq = len(positional_indices.get(item)[0])\n",
    "\n",
    "                idf = 0\n",
    "                if term_frequency!=0:\n",
    "                    idf = log(6000 / term_frequency)\n",
    "                for posting in positional_indices.get(token)[0]:\n",
    "                    doc_id = posting[0]\n",
    "                    term_doc_frequency = len(posting[1])\n",
    "                    if score.__contains__(doc_id):\n",
    "                        score[doc_id] += (idf * (term_doc_frequency * (k1 + 1)) / (term_doc_frequency + k1 * (1 - b + b * len(token) / avg_doc_len)) * weight)\n",
    "                    else:\n",
    "                        score[doc_id] = (idf * (term_doc_frequency * (k1 + 1)) / (term_doc_frequency + k1 * (1 - b + b * len(token) / avg_doc_len)) * weight)\n",
    "    max_idf = {}\n",
    "    if not no_abstract:\n",
    "        for token in abs_tokens:\n",
    "            if positional_indices.__contains__(token):\n",
    "                term_frequency = len(positional_indices.get(token)[1])\n",
    "                preference_freq = len(positional_indices.get(item)[0])\n",
    "\n",
    "                idf =0\n",
    "                if term_frequency !=0:\n",
    "                    idf = log(6000 / term_frequency)\n",
    "                max_idf[token] = idf\n",
    "                for posting in positional_indices.get(token)[1]:\n",
    "                    doc_id = posting[0]\n",
    "                    term_doc_frequency = len(posting[1])\n",
    "                    if score.__contains__(doc_id):\n",
    "                        score[doc_id] += ((1 - personalization_weight) * (idf * (term_doc_frequency * (k1 + 1)) / (term_doc_frequency + k1 * (1 - b + b * len(token) / avg_doc_len)) * (1 - weight)) + personalization_weight * preference_freq)\n",
    "                    else:\n",
    "                        score[doc_id] = ((1 - personalization_weight) * (idf * (term_doc_frequency * (k1 + 1)) / (term_doc_frequency + k1 * (1 - b + b * len(token) / avg_doc_len)) * (1 - weight)) + personalization_weight * preference_freq)\n",
    "    c = Counter(score)\n",
    "    result = c.most_common(max_result_count)\n",
    "    highlighted_result = []\n",
    "\n",
    "\n",
    "    full_corpus = construct_full_corpus()\n",
    "    for doc_id, score in result:\n",
    "        doc = full_corpus[doc_id]\n",
    "        title = doc[\"title\"]\n",
    "\n",
    "        snippet = \" \".join(doc[\"abstract\"].split(\" \")[0: 16])\n",
    "\n",
    "        important_words = []\n",
    "        for word in (abstract_query + title_query):\n",
    "            if word in doc[\"abstract\"]:\n",
    "                important_words.append(word)\n",
    "        \n",
    "        snippet += \"...\" + \"\".join(important_words)\n",
    "        \n",
    "        \n",
    "\n",
    "        highlighted_result.append([doc_id,title,snippet+'...'])\n",
    "    return highlighted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['6cf98b123feac6504b0dc3a8b46e1462dd69121e',\n",
       "  'Data mining: practical machine learning tools and techniques, 3rd Edition',\n",
       "  'Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts...Deep LearningMachine Learning...'],\n",
       " ['7dae942104dc8283504ce7a492c9ca12fa119189',\n",
       "  'Applications, promises, and pitfalls of deep learning for fluorescence image reconstruction',\n",
       "  'Deep learning is becoming an increasingly important tool for image reconstruction in fluorescence microscopy. We review...Deep earningachine earning...'],\n",
       " ['8388f1be26329fa45e5807e968a641ce170ea078',\n",
       "  'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks',\n",
       "  'In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision...Deep earningachine earning...'],\n",
       " ['48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852',\n",
       "  'Deep learning for cellular image analysis',\n",
       "  'Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive...eep earningachine earning...'],\n",
       " ['08dc94471605308669c8d3d8284ba94fcc93e345',\n",
       "  'Deep Learning in Microscopy Image Analysis: A Survey',\n",
       "  'Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning...eep earningMachine earning...'],\n",
       " ['0cbc480e0d380bbaa04bfb21a396c9e8da6e930e',\n",
       "  'Automated analysis of high‐content microscopy data with deep learning',\n",
       "  'Existing computational pipelines for quantitative analysis of high‐content microscopy data rely on traditional machine learning approaches...Deep Learningachine Learning...'],\n",
       " ['00af02c2cb48920af477115e870a42ac4f8a3834',\n",
       "  'Robust feature learning by improved auto-encoder from non-Gaussian noised images',\n",
       "  'Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief...Deep earningMachine earning...'],\n",
       " ['c89bfd998b0a6c656010b629814ab0cad3cff72e',\n",
       "  'Evaluation of Deep Learning Strategies for Nucleus Segmentation in Fluorescence Images',\n",
       "  'Identifying nuclei is often a critical first step in analyzing microscopy images of cells, and classical...Deep earningachine earning...'],\n",
       " ['9f7a89bc9b8ebb7152acacc95a84daead92d8f2c',\n",
       "  'DeepCell 2.0: Automated cloud deployment of deep learning models for large-scale cellular image analysis',\n",
       "  'Deep learning is transforming the ability of life scientists to extract information from images. While these...Deep earningachine earning...'],\n",
       " ['819167ace2f0caae7745d2f25a803979be5fbfae',\n",
       "  'The Limitations of Deep Learning in Adversarial Settings',\n",
       "  'Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches...Deep earningachine earning...']]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def search(preferred_field, title_query, abstract_query, max_result_count = 10, method= 'ltn-lnn', weight = 0.5):\n",
    "    \n",
    "    \n",
    "    if method == 'ltn-lnn':\n",
    "        return ltn_lnn(preferred_field, title_query, abstract_query, max_result_count, weight)\n",
    "\n",
    "\n",
    "    elif method == 'ltc-lnc':\n",
    "        return ltc_lnc(preferred_field, title_query, abstract_query, max_result_count, weight)\n",
    "\n",
    "    elif method == 'okapi25':\n",
    "        return BM25(preferred_field, title_query, abstract_query, max_result_count, weight)\n",
    "\n",
    "\n",
    "search(preferred_field=\"Intelligence\", title_query=\"Machine Learning\", abstract_query=\"Deep Learning\", max_result_count=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>  \n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anne E Carpenter': array([0.00962864]), 'T. Jones': array([0.00814484]), 'D. Sabatini': array([0.00794748]), 'P. Golland': array([0.00789433]), 'M. R. Lamprecht': array([0.00784005]), 'J. Moffat': array([0.00771333]), 'S. Altschuler': array([0.00725945]), 'Lani F. Wu': array([0.00725945]), 'I. Kang': array([0.00664758]), 'Robert A Lindquist': array([0.00664758])}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.sparse.linalg as sla\n",
    "\n",
    "def normalize_dict(dictionary):\n",
    "    norm = 0\n",
    "    for score in dictionary.values():\n",
    "        norm += score\n",
    "    for author in dictionary.keys():\n",
    "        dictionary[author] /= norm\n",
    "\n",
    "\n",
    "def calculate_writer_authority_using_left_eig_vector(file_address, n):\n",
    "    with open(file_address, 'r') as f:\n",
    "        crawled_page = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    authors = {}\n",
    "    row_index = 0\n",
    "    for paper in crawled_page:\n",
    "        for author in paper['Authors']:\n",
    "            if not authors.__contains__(author):\n",
    "                authors[author] = row_index\n",
    "                row_index += 1\n",
    "    author_count = len(authors)\n",
    "    P = np.full((author_count, author_count), 0, dtype=float)\n",
    "\n",
    "    data = {}\n",
    "    for paper in crawled_page:\n",
    "        data[paper['ID']] = paper\n",
    "    crawled_page = None\n",
    "\n",
    "    for paper in data.values():\n",
    "        for reference in paper['References']:\n",
    "            if data.__contains__(reference):\n",
    "                for row in paper['Authors']:\n",
    "                    for column in data[reference]['Authors']:\n",
    "                        P[authors[row]][authors[column]] = 1\n",
    "    X = np.transpose(P)\n",
    "    P = X @ P\n",
    "    eval, evec = sla.eigs(P.T, k=1, which='LM')\n",
    "    a = (evec / evec.sum()).real\n",
    "\n",
    "    for author, row in authors.items():\n",
    "        authors[author] = a[row]\n",
    "    sorted_output = dict(sorted(authors.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "    return sorted_output\n",
    "\n",
    "\n",
    "def calculate_writer_authority_using_iteration(file_address, n):\n",
    "    with open(file_address, 'r') as f:\n",
    "        crawled_page = json.load(f)\n",
    "\n",
    "    data = {}\n",
    "    for paper in crawled_page:\n",
    "        data[paper['ID']] = paper\n",
    "    crawled_page = None\n",
    "\n",
    "    authors = {}  # will contain [{authors that have a reference to this author},{authors that this author has reference to}]\n",
    "    hubs = {}\n",
    "    auths = {}\n",
    "\n",
    "    for paper in data.values():\n",
    "        for author in paper['Authors']:\n",
    "            hubs[author] = 1\n",
    "            auths[author] = 1\n",
    "            for reference in paper['References']:\n",
    "                if data.__contains__(reference):\n",
    "                    for author_reference in data[reference]['Authors']:\n",
    "                        if authors.__contains__(author_reference):\n",
    "                            authors[author_reference][0][author] = True\n",
    "                        else:\n",
    "                            authors[author_reference] = [{author: True}, {}]\n",
    "\n",
    "                        if authors.__contains__(author):\n",
    "                            authors[author][1][author_reference] = True\n",
    "                        else:\n",
    "                            authors[author] = [{}, {author_reference: True}]\n",
    "\n",
    "    iterative_count = 5\n",
    "    while iterative_count > 0:\n",
    "        for author, info in authors.items():\n",
    "            hub = 0\n",
    "            for node in info[1]:\n",
    "                hub += auths[node]\n",
    "            hubs[author] = hub\n",
    "        normalize_dict(hubs)\n",
    "\n",
    "        for author, info in authors.items():\n",
    "            authority = 0\n",
    "            for node in info[0]:\n",
    "                authority += hubs[node]\n",
    "            auths[author] = authority\n",
    "        normalize_dict(auths)\n",
    "\n",
    "        iterative_count -= 1\n",
    "\n",
    "    sorted_output = dict(sorted(auths.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "    return sorted_output\n",
    "\n",
    "\n",
    "print(calculate_writer_authority_using_left_eig_vector('crawled_paper_Rohban.json', 10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
    "\n",
    "در فایل recommended_papers.json\n",
    "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
    "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
    "\n",
    "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('recommended_papers.json', 'r') as fp:\n",
    "    recommended_papers = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_user = recommended_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
      "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
      "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
      "['Computer Science']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['positive_papers'][0]['paperId'])\n",
    "print(sample_user['positive_papers'][0]['title'])\n",
    "print(sample_user['positive_papers'][0]['abstract'])\n",
    "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
      "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
      "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
      "['Computer Science', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['recommendedPapers'][0]['paperId'])\n",
    "print(sample_user['recommendedPapers'][0]['title'])\n",
    "print(sample_user['recommendedPapers'][0]['abstract'])\n",
    "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
    "\n",
    "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
    " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
    "\n",
    "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
    "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = {}\n",
    "Y = {}\n",
    "for i, user in enumerate(recommended_papers):\n",
    "    X[i] = user['positive_papers']\n",
    "    Y[i] = user['recommendedPapers']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "def find_fields(X_train, positive_papers):\n",
    "    unique_field = {}\n",
    "    for paper in positive_papers:\n",
    "      if paper['fieldsOfStudy'] != None:\n",
    "        for field in paper['fieldsOfStudy']:\n",
    "          if field not in unique_field.keys():\n",
    "            unique_field[field] = len(unique_field.keys())\n",
    "\n",
    "    for user in X_train:\n",
    "      for paper in user:\n",
    "        if paper['fieldsOfStudy'] != None:\n",
    "          for field in paper['fieldsOfStudy']:\n",
    "            if field not in unique_field.keys():\n",
    "              unique_field[field] = len(unique_field.keys())\n",
    "    return unique_field\n",
    "\n",
    "\n",
    "def prepare_vectors(X_train, positive_papers, unique_field):\n",
    "    test_vec = np.zeros(len(unique_field.keys()))\n",
    "    for paper in positive_papers:\n",
    "      if paper['fieldsOfStudy'] != None:\n",
    "        for field in paper['fieldsOfStudy']:\n",
    "          test_vec[unique_field[field]] += 1\n",
    "    for i in range(len(test_vec)):\n",
    "        test_vec[i] = test_vec[i] / len(positive_papers)\n",
    "\n",
    "    train_vecs = []\n",
    "    for user in X_train:\n",
    "      train_vec = np.zeros(len(unique_field.keys()))\n",
    "      for paper in user:\n",
    "        if paper['fieldsOfStudy'] != None:\n",
    "          for field in paper['fieldsOfStudy']:\n",
    "            train_vec[unique_field[field]] += 1\n",
    "      for i in range(len(test_vec)):\n",
    "        train_vec[i] = train_vec[i] / len(user)\n",
    "      train_vecs.append(train_vec)\n",
    "    return test_vec, train_vecs\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm1 = np.linalg.norm(vector1)\n",
    "    norm2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def collaborative_filtering(X_train, y_train, positive_papers, N=10):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on similar users (Similar users should be on \"train data\").\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "    N: The number of hyperparameter N in Nearest Neighbor algorithm.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "    unique_field = find_fields(X_train, positive_papers)\n",
    "    test_vec, train_vecs = prepare_vectors(X_train, positive_papers, unique_field)\n",
    "\n",
    "    similarity_scores = []\n",
    "    for train_vector in train_vecs:\n",
    "        similarity = cosine_similarity(test_vec, train_vector)\n",
    "        similarity_scores.append(similarity)\n",
    "\n",
    "    top_indices = np.argsort(similarity_scores)[-N:]\n",
    "    candidate = {}\n",
    "    for indice in top_indices:\n",
    "        for paper in y_train[indice]:\n",
    "            if paper['paperId'] not in candidate.keys():\n",
    "                candidate [paper['paperId']] = 1\n",
    "            else:\n",
    "                candidate [paper['paperId']] += 1\n",
    "\n",
    "    sorted_candidate = sorted(candidate.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    result = [item[0] for item in sorted_candidate ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ba852c774c00894376bc20cdccb884b0dbe1196b', '2714c11c0809d638e1e501831913671914407e5d', '590b8f5e17424d1d4be560a0d2b1c665d8d3c7f8', '5cbadc7545b5296a8b245be20c78f8b9b628973c', '66fa8acfbf6dc1022d5aa2ee43fad20cda231f98', 'b65dfaf9b95b21840848b3b77bb4df655305ac89', '908bdaa3588ac073d06b4452ffd5fce7bd9af042', '3256e193a308e451c7107e51bb96c3e9b5bb6ae3', '33320785835ba8cdd717cb8d043e99e942e0b491', '15b3523eecb9a4035324662bbb77d7d8b7185d5b']\n",
      "ba852c774c00894376bc20cdccb884b0dbe1196b\n",
      "908bdaa3588ac073d06b4452ffd5fce7bd9af042\n",
      "2714c11c0809d638e1e501831913671914407e5d\n",
      "33320785835ba8cdd717cb8d043e99e942e0b491\n",
      "66fa8acfbf6dc1022d5aa2ee43fad20cda231f98\n",
      "b241ab3e988818ce38518e317a2733d9b5103d5f\n",
      "5cbadc7545b5296a8b245be20c78f8b9b628973c\n",
      "600e824cadf41e3543a4c0db22226aeb1578bce9\n",
      "057df6d53a4b7f6314a1d1b8a9f0f3664bd5aa01\n",
      "590b8f5e17424d1d4be560a0d2b1c665d8d3c7f8\n"
     ]
    }
   ],
   "source": [
    "print(collaborative_filtering(X_train, y_train,X_test[0] , N=10))\n",
    "for paper in y_test[0]:\n",
    "    print(paper['paperId'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Content Based (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
    "\n",
    "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
    "\n",
    "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm1 = np.linalg.norm(vector1)\n",
    "    norm2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def prepare_corpus(y_train, positive_papers):\n",
    "    corpus = []\n",
    "    papers = []\n",
    "    for user in y_train:\n",
    "      for paper in user:\n",
    "        if paper['paperId'] not in papers:\n",
    "            papers.append(paper['paperId'])\n",
    "            corpus.append(paper['title'])\n",
    "    for paper in positive_papers:\n",
    "      if paper['paperId'] not in papers:\n",
    "          papers.append(paper['paperId'])\n",
    "          corpus.append(paper['title'])\n",
    "    return corpus\n",
    "\n",
    "def content_based_recommendation(y_train, positive_papers):\n",
    "    \"\"\"\n",
    "    Returns the top 10 related articles to the user, based on the titles of the articles.\n",
    "\n",
    "    Parameters:\n",
    "    user_id (int): The unique index of the user.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of 10 article IDs that are most relevant to the user's interests.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(prepare_corpus(y_train, positive_papers))\n",
    "\n",
    "    test_vec_temp = 0\n",
    "    for paper in positive_papers:\n",
    "        test_vec_temp += np.sum(vectorizer.transform([paper['title']]), axis = 0)\n",
    "    test_vec = np.array(test_vec_temp / len(positive_papers))[0]\n",
    "\n",
    "    train_vecs = []\n",
    "    train_id = []\n",
    "    score = {}\n",
    "    for user in y_train:\n",
    "      for paper in user:\n",
    "        if paper['paperId'] not in train_id:\n",
    "            train_id.append(paper['paperId'])\n",
    "            train_vecs.append(np.array(np.sum(vectorizer.transform([paper['title']]) , axis = 0))[0])\n",
    "            score[paper['paperId']] = cosine_similarity(test_vec , train_vecs[len(train_vecs) - 1])\n",
    "\n",
    "    sorted_score = sorted(score.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    result = [item[0] for item in sorted_score]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12ffb095891a42dd80625667a3234ded06ffa040', 'c39f3752591181899f680644e8f6ce77cc5a8e3e', 'ecc3691792d66a38584f6e6d6b5472427e762cce', '8763ac18bd2af1602d75152e35fac78a7198d68f', 'ee46421f518c17a51145d33a0fe7ccc6da31c51e', '638e508b1a0694c847d3094ad664006b2b761f09', '7d03af1ccf5404e23bee02903b41850e88cc8590', 'a4ed522d3fcacdccc1f0c38a1109ca721fa4490b', 'cbe3519785e86efafa3f4f72e7ff1a0b9398529a', '6625a66e96821efed9b19e81d86bda7d66931020']\n",
      "1d36e7ba19be5db9694ed256ea21dae5f753ede3\n",
      "ebca36a78971096cca26e1e59931dc07bba4b25a\n",
      "a8dac0d0837ac4800f4462a121c59a98a05531ee\n",
      "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
      "c9239097e39968b061973ac97917cacf8d80391a\n",
      "355a88ee6967af23512f85f947270cf9d81ea098\n",
      "e82717bc04fd1143828d86236662aceae24b2d36\n",
      "79f43d149cd569abf46428ed8a27a8a2b3e44a8f\n",
      "70842f8ff3f9284ed98ba8e4e98848c06fde7813\n",
      "7d03af1ccf5404e23bee02903b41850e88cc8590\n"
     ]
    }
   ],
   "source": [
    "print(content_based_recommendation( y_train, X_test[1]))\n",
    "for paper in y_test[1]:\n",
    "    print(paper['paperId'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5233072175813027\n",
      "0.03300768379354474\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dcg_score(ranked_list):\n",
    "    dcg = 0.0\n",
    "    for i in range(len(ranked_list)):\n",
    "        relevance = ranked_list[i]\n",
    "        dcg += (2 ** relevance - 1) / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def ndcg_score(true_list, pred_list):\n",
    "    ideal_dcg = dcg_score(true_list)\n",
    "    pred_dcg = dcg_score(pred_list)\n",
    "    ndcg = pred_dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "    return ndcg\n",
    "\n",
    "def calculate_score(pred , papers):\n",
    "    true = []\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    for paper in papers:\n",
    "        true.append(paper['paperId'])\n",
    "    for id in pred:\n",
    "        if id in true:\n",
    "            pred_list.append(10 - true.index(id))\n",
    "        else:\n",
    "            pred_list.append(0.0001)\n",
    "    for i in range(len(true)):\n",
    "        true_list.append(10 - i)\n",
    "\n",
    "    return ndcg_score(true_list, pred_list)\n",
    "\n",
    "collaborative_sum = 0\n",
    "for i in range(len(y_test)):\n",
    "    collaborative_sum += calculate_score(collaborative_filtering(X_train, y_train, X_test[i] ,N=10) , y_test[i])\n",
    "collaborative_mean = collaborative_sum / len(y_test)\n",
    "print(collaborative_mean)\n",
    "\n",
    "content_based_sum = 0\n",
    "for i in range(len(y_test)):\n",
    "    content_based_sum += calculate_score(content_based_recommendation(y_train, X_test[i]) , y_test[i])\n",
    "content_based_mean = content_based_sum / len(y_test)\n",
    "print(content_based_mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش\n",
    " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### User Interface in UI.py file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
